[
  {
    "objectID": "application/session.html",
    "href": "application/session.html",
    "title": "Browser session",
    "section": "",
    "text": "Concurrency should be taken into consideration in developing our web application. Concurrency refers to the ability of the application to handle multiple tasks or connections simultaneously, without blocking or waiting for other tasks to complete. This is crucial for ensuring that the service remains responsive and scalable, especially when serving a large number of users.\nHowever, with our current framework, there are multiple functions that utilize global variables, which will be shared from the server-side. This can cause significant issues in a concurrent environment, such as race conditions, where multiple tasks or connections attempt to read and modify the same variable simultaneously. This may lead to unpredictable behavior, data corruption, or incorrect results. For instance, if two users’ requests depend on the same global variable being updated, the updates might overwrite each other or produce inconsistent states, affecting the reliability of the application.\nOur current solution to this issue involves introducing the concept of session. Each session represents one connection with a user browser and all session-specific variables are stored in a higher-level mapping of session ID to values. Our frameworks will also configured towards the use of session.",
    "crumbs": [
      "Application",
      "Browser session"
    ]
  },
  {
    "objectID": "application/session.html#session-messages",
    "href": "application/session.html#session-messages",
    "title": "Browser session",
    "section": "Session messages",
    "text": "Session messages\nAs a chatbot application, we usually require the whole conversation being pasted into chat history rather than using a single new message for a smooth user experience. For simplication purpose, we introduce a mock database for storing our messages list for each session:\n\nsource\n\nretrieve_session_message\n\n retrieve_session_message (session_id:str)\n\nRetrieve the messages for a given session_id",
    "crumbs": [
      "Application",
      "Browser session"
    ]
  },
  {
    "objectID": "application/session.html#session-tools",
    "href": "application/session.html#session-tools",
    "title": "Browser session",
    "section": "Session tools",
    "text": "Session tools\nOne major utility in our framework that uses global variables is the llmcam.utils.store module which updates the global tools list. To configure this towards session framework, we can save the tools instance (Python pointer) of each session and define a custom fixup function that retrieves this instance and passes it to the actual manager tools. The schema of these manager tools also require a new metadata field called session_id for the fixup function to identify the correct instance.\n\nsource\n\nretrieve_session_tools\n\n retrieve_session_tools (session_id:str)\n\nRetrieve the tools for a given session_id\n\nsource\n\n\nprepare_handler_schemas\n\n prepare_handler_schemas (session_id:str, fixup:Callable=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsession_id\nstr\n\nSession ID to use\n\n\nfixup\nCallable\nNone\nOptional function to fix up the execution\n\n\n\n\nsource\n\n\nexecute_handler\n\n execute_handler (function_name:str, session_id:str, **kwargs)\n\n\n\n\n\nType\nDetails\n\n\n\n\nfunction_name\nstr\nName of the function to execute\n\n\nsession_id\nstr\nSession ID to use\n\n\nkwargs\nVAR_KEYWORD",
    "crumbs": [
      "Application",
      "Browser session"
    ]
  },
  {
    "objectID": "application/session.html#session-notifications",
    "href": "application/session.html#session-notifications",
    "title": "Browser session",
    "section": "Session notifications",
    "text": "Session notifications\nThe notifications system is quite more complicated. It relies on also subtools to pass in sub GPT in the separate thread running notification stream. These subtools include a tool to send notification and a tool to stop the stream. The default utilities function for this uses a common global pointer stream_thread, which will not work in this case. Hence, we need to implement the new utilities function:\n\nexecute_send_notification: Instead of implementing the actual function, a fixup is implemented to capitalize on the metadata field session_id. This function also utilizes the feature that a websocket sender function can be saved in global collections - example with real time Chat App. Therefore, it simply retrieves the sender function with session_id and uses it to send the message.\n\nexecute_stopper: Instead of implementing the actual function, a fixup is implemented to capitalize on the metadata field session_id and noti_id. The idea is that each notification stream is given a unique ID and saved in a mapping of IDs to notification streams. This mapping is also retrievable by session_id.\n\nWe also need to implement a custom start_notification_stream function which will be used as a tool by GPT Function Calling. This function will utilize session_id and create a unique noti_id to define schemas for subtools with these values as metadata. It also defines these schemas such that the module metadata is missing to ensure the attached fixup function will be called instead. However, because it uses a metadata field session_id, it will also need to have a fixup function.\n\nsource\n\nretrieve_session_notis\n\n retrieve_session_notis (session_id:str)\n\nRetrieve the notification sender and streams for a given session_id\n\nsource\n\n\nset_noti_sender\n\n set_noti_sender (session_id:str, noti_sender:Callable)\n\nSet the notification sender for a given session_id\n\nsource\n\n\nprepare_stopper_schema\n\n prepare_stopper_schema (session_id:str)\n\n\nsource\n\n\nprepare_sender_schema\n\n prepare_sender_schema (session_id:str)\n\n\nsource\n\n\nexecute_send_notification\n\n execute_send_notification (function_name, session_id, msg, **kwargs)\n\nFixup function to send a notification.\n\nsource\n\n\nexecute_stopper\n\n execute_stopper (function_name, session_id, noti_id, **kwargs)\n\nFixup function to stop a notification stream.\nImplementation of custom start_notification_stream:\n\nsource\n\n\nstart_notification_stream\n\n start_notification_stream (session_id:str, messages:list)\n\nStart a notification stream to monitor a process described in messages.\n\n\n\n\nType\nDetails\n\n\n\n\nsession_id\nstr\nSession ID to use\n\n\nmessages\nlist\nAll the previous messages in the conversation\n\n\n\nDefine the fixup function for starting a notification stream which passes in session_id to start_notification_stream. Define also a function to attach session_id as metadata for the notification FC schema and attach the fixup function.\n\nsource\n\n\nexecute_start_notification_stream\n\n execute_start_notification_stream (function_name, session_id, messages,\n                                    **kwargs)\n\nFixup function to start a notification stream.\n\nsource\n\n\nprepare_notification_schemas\n\n prepare_notification_schemas (session_id:str, fixup:Callable=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsession_id\nstr\n\nSession ID to use\n\n\nfixup\nCallable\nNone\nOptional function to fix up the execution",
    "crumbs": [
      "Application",
      "Browser session"
    ]
  },
  {
    "objectID": "application/session.html#setup",
    "href": "application/session.html#setup",
    "title": "Browser session",
    "section": "Setup",
    "text": "Setup\nImplement some utilities to manage all session-related data:\n\nsource\n\ninit_session\n\n init_session (session_id:Optional[str]=None)\n\n\nsource\n\n\nremove_session\n\n remove_session (session_id:str)\n\nRemove the session with the given session_id",
    "crumbs": [
      "Application",
      "Browser session"
    ]
  },
  {
    "objectID": "application/chat_ui.html",
    "href": "application/chat_ui.html",
    "title": "Chat UI in fastHTML",
    "section": "",
    "text": "Start by creating the chat application with FastHTML.\nFor running while testing with Jupyter notebook, use the JupyUvi in fasthtml to run in separate thread.\n\nfrom fasthtml.jupyter import *\n\nserver = JupyUvi(app=app)\n\n\n\n\n\n\nserver.stop()",
    "crumbs": [
      "Application",
      "Chat UI in fastHTML"
    ]
  },
  {
    "objectID": "application/chat_ui.html#chat-app-initialization",
    "href": "application/chat_ui.html#chat-app-initialization",
    "title": "Chat UI in fastHTML",
    "section": "",
    "text": "Start by creating the chat application with FastHTML.\nFor running while testing with Jupyter notebook, use the JupyUvi in fasthtml to run in separate thread.\n\nfrom fasthtml.jupyter import *\n\nserver = JupyUvi(app=app)\n\n\n\n\n\n\nserver.stop()",
    "crumbs": [
      "Application",
      "Chat UI in fastHTML"
    ]
  },
  {
    "objectID": "application/chat_ui.html#chat-components",
    "href": "application/chat_ui.html#chat-components",
    "title": "Chat UI in fastHTML",
    "section": "Chat components",
    "text": "Chat components\nBasic chat UI components can include Chat Message and a Chat Input. For a Chat Message, the important attributes are the actual message (str) and the role of the message owner (user - boolean value whether the owner is the user, not the AI assistant).\n\nsource\n\nChatMessage\n\n ChatMessage (msg:str, user:bool)\n\n\n\n\n\nType\nDetails\n\n\n\n\nmsg\nstr\nMessage to display\n\n\nuser\nbool\nWhether the message is from the user or assistant\n\n\n\nFor the chat input, set the name for submitting a new message via form.\n\nsource\n\n\nChatInput\n\n ChatInput ()\n\n\n\nAction Buttons\nSimple actions for creating a new message from the user side.\n\nsource\n\n\nActionButton\n\n ActionButton (session_id:str, content:str, message:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsession_id\nstr\n\nSession ID to use\n\n\ncontent\nstr\n\nText to display on the button\n\n\nmessage\nstr\nNone\nMessage to send when the button is clicked\n\n\n\n\nsource\n\n\nActionPanel\n\n ActionPanel (session_id:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nsession_id\nstr\nSession ID to use\n\n\n\n\n\nTools panel\nSidebar-panel for displaying current list of available (loaded) tools in a user-session.\n\nsource\n\n\nToolPanel\n\n ToolPanel (session_id:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nsession_id\nstr\nSession ID to use\n\n\n\n\n\nNotifications\nThe idea of sending notifications from a background task / websocket with FastHTML is to send an HTMX update, then detect and extract information from the event via a document event listener.\n\nsource\n\n\nNotiButton\n\n NotiButton (session_id:str)\n\n\n\n\n\nType\nDetails\n\n\n\n\nsession_id\nstr\nSession ID to use\n\n\n\n\nsource\n\n\nNotiMessage\n\n NotiMessage (message:str='No message')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmessage\nstr\nNo message\nMessage to display",
    "crumbs": [
      "Application",
      "Chat UI in fastHTML"
    ]
  },
  {
    "objectID": "application/chat_ui.html#router",
    "href": "application/chat_ui.html#router",
    "title": "Chat UI in fastHTML",
    "section": "Router",
    "text": "Router\n\nHome page\nThe home page should contain our message list and the Chat Input. The main page can be extracted by accessing the index (root) endpoint.\n\ndef click_here(): return a('Click here', href='https://example.com', target='_blank')\n\n\nsource\n\n\nindex\n\n index (session)\n\n\n\nNotification websockets\n\nsource\n\n\nnoti_disconnect\n\n noti_disconnect (ws)\n\nRemove session ID from session notification sender on websocket disconnect\n\nsource\n\n\nwsnoti\n\n wsnoti (ws, send, session_id:str)\n\nTest usage:\n\n#first_noti_sender = list(session_notis.values())[0][0]\n#first_noti_sender(\"Test message.\")\n\n\n\nChat websockets\nWhen connecting to websockets for chat, this function should:\n\nExtract the new and all previous chat history\n\nPrompt & get answers from ChatGPT from all these messages\n\nReturn a new ChatMessage\n\n\nsource\n\n\nchat_disconnect\n\n chat_disconnect (ws)\n\nRemove session ID from session messages and tools on websocket disconnect\n\nsource\n\n\nwschat\n\n wschat (ws, msg:str, send, session_id:str)\n\n\n\nStatic files\nIn case the user needs to display images, serves files from directory ../data.\n\nsource\n\n\nget_file\n\n get_file (file_name:str)\n\nServe files dynamically from the ‘data’ directory.",
    "crumbs": [
      "Application",
      "Chat UI in fastHTML"
    ]
  },
  {
    "objectID": "core/class_to_oas.html",
    "href": "core/class_to_oas.html",
    "title": "class -> OAS",
    "section": "",
    "text": "class Item:\n    def __init__(self, name: str, price: float, quantity: int):\n        self.name = name\n        self.price = price\n        self.quantity = quantity\n\n    def get_total_price(self) -&gt; float:\n        return self.price * self.quantity\n\n\nitem = Item(\"shoes\", 49.0, 2)\nitem.get_total_price()\n\n98.0\n\n\n\nres = api.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are an assistant that generates OpenAPI specs.\"},\n        {\"role\": \"user\", \"content\": content}\n    ]\n)\n\nres = json.loads(res.json())['choices'][0]['message']['content']\nres = res.replace(\"```yaml\", \"\").replace(\"```\", \"\").strip()\n\n/tmp/ipykernel_32782/138741873.py:10: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n  res = json.loads(res.json())['choices'][0]['message']['content']\n\n\n\n#print(res)\n\n\n# !pip install -qq pyyaml openapi-spec-validator\n\n\ndef validate_yaml(yaml_buffer):\n    try:\n        yaml_content = yaml.safe_load(yaml_buffer)\n        print(\"YAML syntax is valid.\")\n        return yaml_content\n    except yaml.YAMLError as e:\n        print(\"YAML syntax error:\", e)\n        return None\n\ndef validate_oas(yaml_content):\n    try:\n        validate_spec(yaml_content)\n        print(\"OpenAPI specification format is valid.\")\n    except Exception as e:\n        print(\"OpenAPI validation error:\", e)\n\n\nyaml_content = validate_yaml(res)\nif yaml_content:\n    validate_oas(yaml_content)\n\nYAML syntax is valid.\nOpenAPI specification format is valid.\n\n\n\nwith open(\"oas.yaml\", \"w\") as f:\n    yaml.dump(yaml_content, f)",
    "crumbs": [
      "Core",
      "class -> OAS"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html",
    "href": "core/oas_to_schema.html",
    "title": "OAS to Tool Schema",
    "section": "",
    "text": "OpenAPI Specification is a standardized format used to describe and document RESTful APIs. The OAS format enables the generation of both human-readable and machine-readable API documentation, making it easier for developers to design, consume, and integrate with APIs.\nIn this context, we can also utilize this for GPT function calling by generating tools schema from this OAS file and implementing a wrapper function to send requests to API server. This implementation focuses on DigiTraffic with its OpenAPI specification file. Summary of the generation process:",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html#download-oas",
    "href": "core/oas_to_schema.html#download-oas",
    "title": "OAS to Tool Schema",
    "section": "Download OAS",
    "text": "Download OAS\nMany OAS files are available or can be easily generated from an API server. We can download such files and open them as Python dictionary.\n\nsource\n\nload_oas\n\n load_oas (oas_url:str, destination:str, overwrite:bool=False)\n\nLoad OpenAPI Specification from URL or file.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\noas_url\nstr\n\nOpenAPI Specification URL\n\n\ndestination\nstr\n\nDestination file\n\n\noverwrite\nbool\nFalse\nOverwrite existing file\n\n\nReturns\ndict\n\nOpenAPI Specification\n\n\n\nLoad OAS from Road DigiTraffic:\n\noas = load_oas(\n    oas_url=\"https://tie.digitraffic.fi/swagger/openapi.json\",\n    destination=\"openapi.json\",\n    overwrite=False\n)\noas['paths'][list(oas['paths'].keys())[0]]\n\n{'get': {'tags': ['Beta'],\n  'summary': 'List the history of sensor values from the weather road station',\n  'operationId': 'weatherDataHistory',\n  'parameters': [{'name': 'stationId',\n    'in': 'path',\n    'description': 'Weather station id',\n    'required': True,\n    'schema': {'type': 'integer', 'format': 'int64'}},\n   {'name': 'from',\n    'in': 'query',\n    'description': 'Fetch history after given date time',\n    'required': False,\n    'schema': {'type': 'string', 'format': 'date-time'}},\n   {'name': 'to',\n    'in': 'query',\n    'description': 'Limit history to given date time',\n    'required': False,\n    'schema': {'type': 'string', 'format': 'date-time'}}],\n  'responses': {'200': {'description': 'Successful retrieval of weather station data',\n    'content': {'application/json;charset=UTF-8': {'schema': {'type': 'array',\n       'items': {'$ref': '#/components/schemas/WeatherSensorValueHistoryDto'}}}}},\n   '400': {'description': 'Invalid parameter(s)'}}}}",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html#deep-reference-extraction",
    "href": "core/oas_to_schema.html#deep-reference-extraction",
    "title": "OAS to Tool Schema",
    "section": "Deep reference extraction",
    "text": "Deep reference extraction\nIn order to generate tool schemas, we need to resolve and flatten the references to components. This process is complicated because of the nested references in components, which may also include circular references and lead to efficiency issues.\nTo address these problems, the implemented algorithm involves:\n\nTraverse components and retrieve:\n\n\nList of all reference names in format components/{section}/{item}.\n\nMapping of reference names to all locations where they are referenced by other references (nested reference). Locations are saved as strings in which each layer is separated by /.\n\nMapping of reference names to all other references they refer to in their definitions (nested references) - dependencies mapping.\n\n\nCheck for “clean” references - references that do not include nested references in their definitions / references that do not have any dependencies.\nAttach the dictionary definitions of the “clean” references to locations that they are referred to.\nUpdate the dependencies mapping and “clean” references list. Repeat step 2, 3, and 4 until all references are “clean” or no more references can be “cleaned” (circular referencing).\nReturn a flat mapping of global reference names (formatted #/components/{section}/{item}) to their expanded (resolved) definitions.\n\n\nmm(\"\"\"\nflowchart TD\n   OC[OAS Components] --&gt;|traverse| RL[Reference list]\n   OC --&gt;|traverse| NRD[Nested reference dependencies]\n   OC --&gt;|traverse| NRL[Nested reference locations]\n   NRD --&gt;|check| CR[Clean references, i.e., no dependencies]\n   CR --&gt;|compare| RL\n   RL --&gt;|Clean &lt; All| CR[Clean references]\n   subgraph Reference resolution\n   CR --&gt;|attach| NRL\n   NRL --&gt;|update| NRD\n   end\n   RL --&gt;|Clean = All| EC[Flattened components]\n   RL --&gt;|Clean unchanged| EC\n\"\"\")\n\n\n    \n        \n    \n    \n\n\nNOTE: For reference locations, layers being separated by / may overlap with MIME types such as text/plain and application/json. Therefore, we need an utility function to scan for these MIME types and extract the correct layers:\n\nsource\n\nretrieve_ref_parts\n\n retrieve_ref_parts (refs:str)\n\nRetrieve the parts of a reference string\nImplementation of the described algorithm:\n\nsource\n\n\nextract_refs\n\n extract_refs (oas:dict)\n\n\n\n\n\nType\nDetails\n\n\n\n\noas\ndict\nThe OpenAPI schema\n\n\nReturns\ndict\nThe extracted references (flattened)\n\n\n\nWe can demonstrate an example usage with Road DigiTraffic. The API server includes a reference called Address which involves nested referencing and circular referencing. The algorithm successfully expands possible parts of this reference and ignores the circular referencing.\n\nrefs = extract_refs(oas)\nrefs[\"#/components/schemas/Address\"]\n\n{'type': 'object',\n 'properties': {'postcode': {'type': 'string'},\n  'city': {'required': ['values'],\n   'type': 'object',\n   'properties': {'values': {'required': ['values'],\n     'type': 'object',\n     'properties': {'values': {'type': 'array',\n       'xml': {'name': 'value',\n        'namespace': 'http://datex2.eu/schema/3/common'},\n       'items': {'type': 'object',\n        'properties': {'value': {'type': 'string'},\n         'lang': {'type': 'string', 'xml': {'attribute': True}}}}}}}}},\n  'countryCode': {'type': 'string'},\n  'addressLines': {'type': 'array',\n   'xml': {'name': 'addressLine'},\n   'items': {'$ref': '#/components/schemas/AddressLine'}},\n  'get_AddressExtension': {'$ref': '#/components/schemas/_ExtensionType'}}}",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html#oas-schema-to-gpt-compatible-schema",
    "href": "core/oas_to_schema.html#oas-schema-to-gpt-compatible-schema",
    "title": "OAS to Tool Schema",
    "section": "OAS schema to GPT-compatible schema",
    "text": "OAS schema to GPT-compatible schema\nGPT currently recognizes only a limited number of descriptors when defining tools schema. Some of these descriptors (fields) can be directly transferred from OAS schema to tools, but many existing OAS schema fields will not be recognized by GPT and can cause errors. Therefore, transformation from OAS schemas to GPT-compatible schemas is necessary.\nGPT currently recognizes these fields:\n\ntype\nSpecifies the data type of the value. Common types include:\n\n\n\nstring – A text string.\n\nnumber – A numeric value (can be integer or floating point).\n\ninteger – A whole number.\n\nboolean – A true/false value.\n\narray – A list of items (you can define the type of items in the array as well).\n\nobject – A JSON object (with properties, which can be further defined with their own types).\n\nnull – A special type to represent a null or absent value.\n\nany – Allows any type, typically used for flexible inputs or outputs.\n\n\n\ndefault: Provides a default value for the field if the user doesn’t supply one. It can be any valid type based on the expected schema.\n\nenum: Specifies a list of acceptable values for a field. It restricts the input to one of the predefined values in the array.\n\nproperties: Used for objects, this defines the subfields of an object and their respective types.\n\nitems: Defines the type of items in an array. For example, you can specify that an array contains only strings or integers.\n\nminLength, maxLength: Specifies minimum and maximum lengths for string parameters.\n\nminItems, maxItems: Specifies mininum and maximum number of items for array parameters.\n\npattern: Specifies a regular expression that the string must match for string parameters.\n\nrequired: A list of required fields for an object. Specifies that certain fields within an object must be provided.\n\nadditionalProperties: Specifies whether additional properties are allowed in an object. If set to false, no properties outside of those defined in properties will be accepted.\n\nAs such, we can extract corresponding fields from OAS schema, and converts all additional fields into parameter description.\n\nsource\n\ntransform_property\n\n transform_property (prop:dict, flatten_refs:dict={})\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprop\ndict\n\nThe property to transform\n\n\nflatten_refs\ndict\n{}\nThe flattened references\n\n\nReturns\ntuple\n\nThe transformed property and whether it is a required property\n\n\n\nTest usage with complex parameters from DigiTraffic endpoints:\n\nparameters = [\n    {\n        \"name\": \"lastUpdated\",\n        \"in\": \"query\",\n        \"description\": \"If parameter is given result will only contain update status.\",\n        \"required\": False,\n            \"schema\": {\n                \"type\": \"boolean\",\n                \"default\": False\n            }\n    },\n    {\n        \"name\": \"roadNumber\",\n        \"in\": \"query\",\n        \"description\": \"Road number\",\n        \"required\": False,\n        \"schema\": {\n            \"type\": \"integer\",\n            \"format\": \"int32\"\n        }\n    },\n    {\n        \"name\": \"xMin\",\n        \"in\": \"query\",\n        \"description\": \"Minimum x coordinate (longitude) Coordinates are in WGS84 format in decimal degrees. Values between 19.0 and 32.0.\",\n        \"required\": False,\n        \"schema\": {\n            \"maximum\": 32,\n            \"exclusiveMaximum\": False,\n            \"minimum\": 19,\n            \"exclusiveMinimum\": False,\n            \"type\": \"number\",\n            \"format\": \"double\",\n            \"default\": 19\n        }\n    }\n]\n\nfor param in parameters:\n    param, required = transform_property(param)\n    print(param)\n    print(f\"Required: {required}\\n\")\n    print()\n\n{'description': 'If parameter is given result will only contain update status. (Name: lastUpdated; In: query)', 'type': 'boolean', 'default': False}\nRequired: False\n\n\n{'description': 'Road number (Name: roadNumber; In: query; Format: int32)', 'type': 'integer'}\nRequired: False\n\n\n{'description': 'Minimum x coordinate (longitude) Coordinates are in WGS84 format in decimal degrees. Values between 19.0 and 32.0. (Name: xMin; In: query; Maximum: 32; Exclusivemaximum: False; Minimum: 19; Exclusiveminimum: False; Format: double)', 'type': 'number', 'default': 19}\nRequired: False",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html#oas-to-schema",
    "href": "core/oas_to_schema.html#oas-to-schema",
    "title": "OAS to Tool Schema",
    "section": "OAS to schema",
    "text": "OAS to schema\nWe can combine the above utilities to extract important information about the functions and creates a GPT-compatible tools schema. The idea is to convert all necessary information for generating an API request to a parameter for GPT to provide. As such, the parameters of each function in this tools schema will include:\n\npath: dictionary for path parameters that maps parameter names to schema\nquery: dictionary for query parameters that maps parameter names to schema\nbody: request body schema\n\nApart from the parameters that GPT should provide, we also need constant values for each function (endpoint). These values should be saved as metadata in tools schema:\n\nurl: URL to send requests to\n\nmethod: HTTP method for each endpoint\n\n\nAuxiliary fixup function\nTo avoid writing Python functions for each of these endpoints, we can use a universal fixup (wrapper) function to execute API requests based on the above data.\nNOTE: Apart from the above inputs, implementation of this fixup function will be more simple with an extra metadata for acceptable query parameters. This is because GPT often flattens parameter inputs for simple requests, making it difficult to differentiate between path and query parameters.\n\nsource\n\n\ngenerate_request\n\n generate_request (function_name:str, url:str, method:str, path:dict={},\n                   query:dict={}, body:dict={}, accepted_queries:list=[],\n                   **kwargs)\n\nGenerate a request from the function name and parameters.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction_name\nstr\n\nThe name of the function\n\n\nurl\nstr\n\nThe URL of the request\n\n\nmethod\nstr\n\nThe method of the request\n\n\npath\ndict\n{}\nThe path parameters of the request\n\n\nquery\ndict\n{}\nThe query parameters of the request\n\n\nbody\ndict\n{}\nThe body of the request\n\n\naccepted_queries\nlist\n[]\nThe accepted queries of the request\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\ndict\n\nThe response of the request\n\n\n\n\n\nAPI Schema\n\nsource\n\n\napi_schema\n\n api_schema (base_url:str, oas:dict, service_name:Optional[str]=None,\n             fixup:Optional[Callable]=None)\n\nForm the tools schema from the OpenAPI schema.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbase_url\nstr\n\nThe base URL of the API\n\n\noas\ndict\n\nThe OpenAPI schema\n\n\nservice_name\nOptional\nNone\nThe name of the service\n\n\nfixup\nOptional\nNone\nFixup function to execute a REST API request\n\n\nReturns\ndict\n\nThe api schema\n\n\n\nExample with Road DigiTraffic:\n\ntools = api_schema(\n    base_url=\"https://tie.digitraffic.fi\",\n    oas=oas,\n    service_name=\"Traffic Information\",\n    fixup=generate_request\n)\ntools[0]\n\n{'type': 'function',\n 'function': {'name': 'weatherDataHistory',\n  'description': 'List the history of sensor values from the weather road station',\n  'parameters': {'type': 'object',\n   'properties': {'path': {'type': 'object',\n     'properties': {'stationId': {'description': 'Weather station id (Name: stationId; In: path; Format: int64)',\n       'type': 'integer'}},\n     'required': ['stationId']},\n    'query': {'type': 'object',\n     'properties': {'from': {'description': 'Fetch history after given date time (Name: from; In: query; Format: date-time)',\n       'type': 'string'},\n      'to': {'description': 'Limit history to given date time (Name: to; In: query; Format: date-time)',\n       'type': 'string'}},\n     'required': []}},\n   'required': ['path']},\n  'metadata': {'url': 'https://tie.digitraffic.fi/api/beta/weather-history-data/{stationId}',\n   'method': 'get',\n   'accepted_queries': ['from', 'to'],\n   'service': 'Traffic Information'},\n  'fixup': '__main__.generate_request'}}",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "core/oas_to_schema.html#simulated-gpt-workflow",
    "href": "core/oas_to_schema.html#simulated-gpt-workflow",
    "title": "OAS to Tool Schema",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework:\n\nfrom llmcam.core.fc import *\n\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n    (\"user\", \"Get the weather camera information for the stations with ID C01503 and C01504.\"),\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nGet the weather camera information for the stations with ID C01503 and C01504.\n&gt;&gt; Assistant:\nHere is the weather camera information for the stations with ID C01503 and C01504:  ### Station\nC01503 - **Name**: Road 51 Inkoo (Tie 51 Inkoo in Finnish) - **Location**: Municipality of Inkoo in\nUusimaa province - **Coordinates**: Latitude 60.05374, Longitude 23.99616 - **Camera Type**: BOSCH -\n**Nearest Weather Station ID**: 1013 - **Collection Status**: Gathering - **Updated Time**:\n2024-12-10T15:25:40Z - **Collection Interval**: Every 600 seconds - **Preset Images**:   -\nInkooseen: [![Image](https://weathercam.digitraffic.fi/C0150301.jpg)](https://weathercam.digitraffic\n.fi/C0150301.jpg)   - Hankoon: [![Image](https://weathercam.digitraffic.fi/C0150302.jpg)](https://we\nathercam.digitraffic.fi/C0150302.jpg)   - Tienpinta: [![Image](https://weathercam.digitraffic.fi/C01\n50309.jpg)](https://weathercam.digitraffic.fi/C0150309.jpg)  ### Station C01504 - **Name**: Road 2\nKarkkila, Kappeli (Tie 2 Karkkila, Kappeli in Finnish) - **Location**: Municipality of Karkkila in\nUusimaa province - **Coordinates**: Latitude 60.536727, Longitude 24.235601 - **Camera Type**:\nHIKVISION - **Nearest Weather Station ID**: 1052 - **Collection Status**: Gathering - **Updated\nTime**: 2024-12-10T15:27:01Z - **Collection Interval**: Every 600 seconds - **Preset Images**:   -\nPoriin: [![Image](https://weathercam.digitraffic.fi/C0150401.jpg)](https://weathercam.digitraffic.fi\n/C0150401.jpg)   - Helsinkiin: [![Image](https://weathercam.digitraffic.fi/C0150402.jpg)](https://we\nathercam.digitraffic.fi/C0150402.jpg)   - Tienpinta: [![Image](https://weathercam.digitraffic.fi/C01\n50409.jpg)](https://weathercam.digitraffic.fi/C0150409.jpg)  If you have any more questions or need\nfurther assistance, feel free to ask!",
    "crumbs": [
      "Core",
      "OAS to Tool Schema"
    ]
  },
  {
    "objectID": "utils/file_manager.html",
    "href": "utils/file_manager.html",
    "title": "File Manager",
    "section": "",
    "text": "Our application occasionally needs to download files with their public URL. Therefore, an utility of our local file manager is downloading online files.\n\nsource\n\n\n\n download_file (url:str, save_path:Optional[str]=None)\n\nDownload a file from a give url\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nURL to download\n\n\nsave_path\nOptional\nNone\nFile name to save\n\n\n\nExample usage with downloading file from DigiTraffic weather camera:\n\nfrom IPython.display import Image\nfile = os.getenv(\"LLMCAM_DATA\", \"../data\") + \"/C0150200.jpg\"\n\ndownload_file(\"https://weathercam.digitraffic.fi/C0150200.jpg\", file)\nImage(file)",
    "crumbs": [
      "Utils",
      "File Manager"
    ]
  },
  {
    "objectID": "utils/file_manager.html#download-files",
    "href": "utils/file_manager.html#download-files",
    "title": "File Manager",
    "section": "",
    "text": "Our application occasionally needs to download files with their public URL. Therefore, an utility of our local file manager is downloading online files.\n\nsource\n\n\n\n download_file (url:str, save_path:Optional[str]=None)\n\nDownload a file from a give url\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nURL to download\n\n\nsave_path\nOptional\nNone\nFile name to save\n\n\n\nExample usage with downloading file from DigiTraffic weather camera:\n\nfrom IPython.display import Image\nfile = os.getenv(\"LLMCAM_DATA\", \"../data\") + \"/C0150200.jpg\"\n\ndownload_file(\"https://weathercam.digitraffic.fi/C0150200.jpg\", file)\nImage(file)",
    "crumbs": [
      "Utils",
      "File Manager"
    ]
  },
  {
    "objectID": "utils/file_manager.html#list-local-files",
    "href": "utils/file_manager.html#list-local-files",
    "title": "File Manager",
    "section": "List local files",
    "text": "List local files\nFile manager should be able to list downloaded / generated data files stored in local data directory. There are four types of files that are used in our application:\n\nImage files: Images captured from Youtube Live capturing or weather cameras. The file-naming scheme is cap_%Y.%m.%d_%H:%M:%S_&lt;place name&gt;.jpg.\n\nDetection files: Images with bounding boxes generated by Object-detection models. File names should start with detection_.\n\nDemo files: Parking lot frames. File names should start with demo3_.\n\nPlot files: Plots generated by GPT Function calling. File names should end with plot.\n\n\ndata_path = os.getenv(\"LLMCAM_DATA\", \"../data\")\nfiles = sorted(glob.glob(f\"{data_path}/cap_*.jpg\"))\n\n\nlen(files), files[:3], type(files[0])\n\n(561,\n ['/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_15:59:06_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:00:11_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:01:16_Etelasatama.jpg'],\n str)\n\n\n\nsource\n\nlist_image_files\n\n list_image_files ()\n\nList all captured image files. file naming scheme is “cap_%Y.%m.%d_%H:%M:%S_.jpg”\n\nsource\n\n\nlist_detection_files\n\n list_detection_files ()\n\nList all detection images. File name starts with detection_\n\nsource\n\n\nlist_demo3_files\n\n list_demo3_files ()\n\nList all parking lot frames. File name starts with demo3_\n\nsource\n\n\nlist_plot_files\n\n list_plot_files ()\n\nList all plots. File name ends with plot\nExample usage:\n\nimages = list_image_files().split(\"\\n\")\ndetections = list_detection_files().split(\"\\n\")\nplots = list_plot_files().split(\"\\n\")\n\nprint(\"Example image file:\", images[0])\nprint(\"Example detection file:\", detections[0])\nprint(\"Example plot file:\", plots[0])\n\nExample image file: /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.11.20_13:21:57_Porvoo_C0150200.jpg\nExample detection file: /home/nghivo/tinyMLaaS/llmcam/data/detection_cap_2024.12.11_13:59:40_santaclausvillage.jpg\nExample plot file: /home/nghivo/tinyMLaaS/llmcam/data/118_object_count_plot.jpg",
    "crumbs": [
      "Utils",
      "File Manager"
    ]
  },
  {
    "objectID": "utils/file_manager.html#simulated-gpt-workflow",
    "href": "utils/file_manager.html#simulated-gpt-workflow",
    "title": "File Manager",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ntools = [\n    function_schema(download_file, \"Download file\"),\n    function_schema(list_image_files, \"List image files\")\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Download a file from https://weathercam.digitraffic.fi/C0150200.jpg.\"),\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nDownload a file from https://weathercam.digitraffic.fi/C0150200.jpg.\n&gt;&gt; Assistant:\nThe file has been successfully downloaded from [this\nlink](https://weathercam.digitraffic.fi/C0150200.jpg) and saved as \"C0150200.jpg\".\n\n\n\nfrom IPython.display import Image\nImage(\"C0150200.jpg\")\n\n\n\n\n\n\n\n\n\nmessages.append(form_msg(\"user\", \"List all captured images at Kauppatori in September.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nDownload a file from https://weathercam.digitraffic.fi/C0150200.jpg.\n&gt;&gt; Assistant:\nThe file has been successfully downloaded from [this\nlink](https://weathercam.digitraffic.fi/C0150200.jpg) and saved as \"C0150200.jpg\".\n&gt;&gt; User:\nList all captured images at Kauppatori in September.\n&gt;&gt; Assistant:\nHere are the captured images at Kauppatori in September:  1. **September 28th:**    - [cap_2024.09.2\n8_20:07:56_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_20:07:56_Kauppa\ntori.jpg)    - [cap_2024.09.28_20:23:16_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/c\nap_2024.09.28_20:23:16_Kauppatori.jpg)  2. **September 29th:**    - [cap_2024.09.29_19:20:31_Kauppat\nori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.29_19:20:31_Kauppatori.jpg)    - [ca\np_2024.09.29_19:21:31_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.29_19:2\n1:31_Kauppatori.jpg)  3. **September 30th:**    - [cap_2024.09.30_09:25:01_Kauppatori.jpg](sandbox:/\nhome/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.30_09:25:01_Kauppatori.jpg)    - [cap_2024.09.30_09:21\n:21_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.30_09:21:21_Kauppatori.jp\ng)    - [cap_2024.09.30_09:40:46_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024\n.09.30_09:40:46_Kauppatori.jpg)  These files are available from the listed captured dates in\nSeptember.\n\n\n\nmessages.append(form_msg(\"user\", \"List all locations in the captured images.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nDownload a file from https://weathercam.digitraffic.fi/C0150200.jpg.\n&gt;&gt; Assistant:\nThe file has been successfully downloaded from [this\nlink](https://weathercam.digitraffic.fi/C0150200.jpg) and saved as \"C0150200.jpg\".\n&gt;&gt; User:\nList all captured images at Kauppatori in September.\n&gt;&gt; Assistant:\nHere are the captured images at Kauppatori in September:  1. **September 28th:**    - [cap_2024.09.2\n8_20:07:56_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_20:07:56_Kauppa\ntori.jpg)    - [cap_2024.09.28_20:23:16_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/c\nap_2024.09.28_20:23:16_Kauppatori.jpg)  2. **September 29th:**    - [cap_2024.09.29_19:20:31_Kauppat\nori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.29_19:20:31_Kauppatori.jpg)    - [ca\np_2024.09.29_19:21:31_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.29_19:2\n1:31_Kauppatori.jpg)  3. **September 30th:**    - [cap_2024.09.30_09:25:01_Kauppatori.jpg](sandbox:/\nhome/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.30_09:25:01_Kauppatori.jpg)    - [cap_2024.09.30_09:21\n:21_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.30_09:21:21_Kauppatori.jp\ng)    - [cap_2024.09.30_09:40:46_Kauppatori.jpg](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024\n.09.30_09:40:46_Kauppatori.jpg)  These files are available from the listed captured dates in\nSeptember.\n&gt;&gt; User:\nList all locations in the captured images.\n&gt;&gt; Assistant:\nHere are all the locations captured in the images:  1. **Porvoo** 2. **Valkosaari** 3.\n**Tuomiokirkko** 4. **Kauppatori** 5. **Torni** 6. **Presidentinlinna** 7. **Etelasatama** 8.\n**Olympiaterminaali** 9. **None** 10. **Unclear** 11. **Santa Claus Village** 12. **Kehä 1 Itään**\n13. **Kuopioon**  These are the distinct locations found based on the captured images' file names.",
    "crumbs": [
      "Utils",
      "File Manager"
    ]
  },
  {
    "objectID": "utils/notification.html",
    "href": "utils/notification.html",
    "title": "Notification",
    "section": "",
    "text": "Sending notifications is an important feature for our application. However, because this monitoring process should be initiated by the user via Chat GPT, the condition for sending notifications or stopping the monitoring stream is not constant. Therefore, we can attempt dynamically checking the conditions by using nested GPT Function calling.\nIn nested GPT FC, the notification process will start an on-going loop for checking the condition for sending notification. The condition itself is not explicitly hard-coded in the code but decided by a sub GPT Agent in the loop. This sub GPT FC process will take in the previous messages which contain the description for the notification task. This sub GPT will also be provided with another tool for sending notifications, which will be called if the condition as described is satisfied in the current loop.\n\n\n\n    \n        \n    \n    \n\n\nAn example scenario is presented as below. Supposing that we have a function (tool) for generating random integers between 1 and 100. We want to be notified every time the function generates a number higher than 50.\n\nimport random\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ndef random_generator():\n    \"\"\"Generate a random number between 1 and 100\"\"\"\n    return random.randint(1, 100)\n\ntools = [function_schema(random_generator)]\n\nDefine tool for sending notifications. In this simplified context, this function only adds the notification message to an existing list named notifications.\n\nnotifications = []\ndef send_notification(msg: str):\n    \"\"\"Send a notification\"\"\"\n    notifications.append(msg)\n    return notifications\n\nTest function for starting the monitoring stream:\n\nimport time\n\ndef start_notification_stream(\n    messages: list  # Previous conversation with the user\n):\n    \"\"\"Start a notification stream\"\"\"\n    subtools = [ tool for tool in tools ]\n    subtools.append(function_schema(send_notification))\n\n    for _ in range(5):  # Only loop up to 5 times for the demo\n        complete(messages, tools=subtools)\n        time.sleep(5)\n\n\nstart_notification_stream(\n    messages = form_msgs([\n        ('user', 'Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.')\n    ])\n)\n\n\nnotifications\n\n['Generated number is 61, which is higher than 50.',\n 'Generated number is 94, which is higher than 50.',\n 'Generated number is 86, which is higher than 50.',\n 'Generated number is 59, which is higher than 50.',\n 'Generated number is 100, which is higher than 50.',\n 'Generated number is 58, which is higher than 50.',\n 'Generated number is 88, which is higher than 50.',\n 'Generated number is 82, which is higher than 50.',\n 'Generated number is 73, which is higher than 50.',\n 'Generated number is 84, which is higher than 50.']",
    "crumbs": [
      "Utils",
      "Notification"
    ]
  },
  {
    "objectID": "utils/notification.html#nested-gpt-function-calling",
    "href": "utils/notification.html#nested-gpt-function-calling",
    "title": "Notification",
    "section": "",
    "text": "Sending notifications is an important feature for our application. However, because this monitoring process should be initiated by the user via Chat GPT, the condition for sending notifications or stopping the monitoring stream is not constant. Therefore, we can attempt dynamically checking the conditions by using nested GPT Function calling.\nIn nested GPT FC, the notification process will start an on-going loop for checking the condition for sending notification. The condition itself is not explicitly hard-coded in the code but decided by a sub GPT Agent in the loop. This sub GPT FC process will take in the previous messages which contain the description for the notification task. This sub GPT will also be provided with another tool for sending notifications, which will be called if the condition as described is satisfied in the current loop.\n\n\n\n    \n        \n    \n    \n\n\nAn example scenario is presented as below. Supposing that we have a function (tool) for generating random integers between 1 and 100. We want to be notified every time the function generates a number higher than 50.\n\nimport random\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ndef random_generator():\n    \"\"\"Generate a random number between 1 and 100\"\"\"\n    return random.randint(1, 100)\n\ntools = [function_schema(random_generator)]\n\nDefine tool for sending notifications. In this simplified context, this function only adds the notification message to an existing list named notifications.\n\nnotifications = []\ndef send_notification(msg: str):\n    \"\"\"Send a notification\"\"\"\n    notifications.append(msg)\n    return notifications\n\nTest function for starting the monitoring stream:\n\nimport time\n\ndef start_notification_stream(\n    messages: list  # Previous conversation with the user\n):\n    \"\"\"Start a notification stream\"\"\"\n    subtools = [ tool for tool in tools ]\n    subtools.append(function_schema(send_notification))\n\n    for _ in range(5):  # Only loop up to 5 times for the demo\n        complete(messages, tools=subtools)\n        time.sleep(5)\n\n\nstart_notification_stream(\n    messages = form_msgs([\n        ('user', 'Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.')\n    ])\n)\n\n\nnotifications\n\n['Generated number is 61, which is higher than 50.',\n 'Generated number is 94, which is higher than 50.',\n 'Generated number is 86, which is higher than 50.',\n 'Generated number is 59, which is higher than 50.',\n 'Generated number is 100, which is higher than 50.',\n 'Generated number is 58, which is higher than 50.',\n 'Generated number is 88, which is higher than 50.',\n 'Generated number is 82, which is higher than 50.',\n 'Generated number is 73, which is higher than 50.',\n 'Generated number is 84, which is higher than 50.']",
    "crumbs": [
      "Utils",
      "Notification"
    ]
  },
  {
    "objectID": "utils/notification.html#modularized-notification-workflow-with-separated-streamthread",
    "href": "utils/notification.html#modularized-notification-workflow-with-separated-streamthread",
    "title": "Notification",
    "section": "Modularized notification workflow with separated StreamThread",
    "text": "Modularized notification workflow with separated StreamThread\nIn the demo above, this function is not yet compatible with our current GPT FC framework. One approach to modularize this function and incorporate parallel execution is to use Python threading library. We can define a custom Thread class to initiate the monitoring process.\n\nsource\n\nStreamThread\n\n StreamThread (thread_id:int, tools:list, messages:list)\n\nA class to run a notification stream in a separate thread\n\n\n\n\nType\nDetails\n\n\n\n\nthread_id\nint\nThe thread ID\n\n\ntools\nlist\nList of tools for sub GPT\n\n\nmessages\nlist\nPrevious conversation with the user\n\n\n\nApart from a function to send notifications, we may also need a function to halt the notification stream. This function may heavily depend on the context of usage (e.g., scripts, web services, …). Therefore, we can define a pair of starting and stopping a stream thread, with an example being the default functions that can be easily used in a Python script:\n\nsource\n\n\ndefault_stream_stopper\n\n default_stream_stopper ()\n\nDefault function to stop the notifications stream\n\nsource\n\n\ndefault_stream_starter\n\n default_stream_starter (tools, messages)\n\nDefault function to start the notifications stream\nSimilar to store utilities, the function for starting a notification stream needs to be attached to multiple context-dependent variables (tools, stream starter and stopper, notifications sender, …). Therefore, we need to define the _core function rather than a hard-coded stream function.\n\nsource\n\n\nnotification_stream_core\n\n notification_stream_core (tools:list, messages:list,\n                           stream_starter:Optional[Callable]=None,\n                           send_notification:Optional[Callable]=None,\n                           stream_stopper:Optional[Callable]=None,\n                           send_notification_schema:Optional[dict]=None,\n                           stream_stopper_schema:Optional[dict]=None)\n\nCore function to start and stop the notifications stream\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntools\nlist\n\nTools to use\n\n\nmessages\nlist\n\nPrevious conversation with the user\n\n\nstream_starter\nOptional\nNone\nFunction to start the stream\n\n\nsend_notification\nOptional\nNone\nFunction to send the notification\n\n\nstream_stopper\nOptional\nNone\nFunction to stop the stream\n\n\nsend_notification_schema\nOptional\nNone\nSchema for the send_notification function\n\n\nstream_stopper_schema\nOptional\nNone\nSchema for the stream_stopper function\n\n\nReturns\nstr\n\n\n\n\n\nAn issue with the current llmcam.core.fn_to_schema is that the type suggestion does not cover complicated nested typings. In this case, the list of messages should be of a specific format to be inputted to OpenAI API:\n[\n    {\n        \"role\": \"user\",\n        \"content\": \"Can you notify me if ...\"\n    }\n]\nHence, the current work-around this issue is to manually adjust the schema of start_notification_stream function:\n\nsource\n\n\nprocess_notification_schema\n\n process_notification_schema (start_notifications_stream:Callable)\n\nProcess the notification schema\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstart_notifications_stream\nCallable\nFunction to start the notifications stream",
    "crumbs": [
      "Utils",
      "Notification"
    ]
  },
  {
    "objectID": "utils/notification.html#simulated-gpt-workflow",
    "href": "utils/notification.html#simulated-gpt-workflow",
    "title": "Notification",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework. This process follows the above demo.\n\nimport random\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ndef random_generator():\n    \"\"\"Generate a random number between 1 and 100\"\"\"\n    return random.randint(1, 100)\n\ntools = [function_schema(random_generator)]\n\n\nnotifications = []\ndef send_notification(msg: str):\n    \"\"\"Send a notification\"\"\"\n    notifications.append(msg)\n    return notifications\n\n\ndef start_notification_stream(\n    messages: list  # Previous conversation with the user\n):\n    return notification_stream_core(\n        tools, \n        messages,\n        stream_starter=default_stream_starter,\n        send_notification=send_notification,\n        stream_stopper=default_stream_stopper\n    )\n\n\ntools.append(process_notification_schema(start_notification_stream))\n\n\nmessages = form_msgs([\n    ('system', 'You are a helpful system administrator. Use the supplied tools to assist the user. \\\nIf asked to monitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream instead.'),\n    ('user', 'Can you notify me every time you generate a number higher than 50? Stop after 10 notifications.'),\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user. If asked to\nmonitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream\ninstead.\n&gt;&gt; User:\nCan you notify me every time you generate a number higher than 50? Stop after 10 notifications.\n&gt;&gt; Assistant:\nSure, I'll begin generating random numbers and will notify you each time a number higher than 50 is\ngenerated, up to a total of 10 notifications. The process is underway.\n\n\nChecking the notifications:\n\nfor noti in notifications:\n    print(noti)\n\nGenerated a number higher than 50: 53\nGenerated a number higher than 50: 63\nGenerated a number higher than 50: 97\nGenerated a number higher than 50: 82\nGenerated a number higher than 50: 65\nGenerated a number higher than 50: 82\nGenerated a number higher than 50: 74\nGenerated a number higher than 50: 98\nGenerated a number higher than 50: 82\nGenerated a number higher than 50: 86\n\n\n\nlen(notifications)\n\n10\n\n\nChecking the global stream thread:\n\nstream_thread.is_alive()\n\nFalse",
    "crumbs": [
      "Utils",
      "Notification"
    ]
  },
  {
    "objectID": "demos/04_railway_example.html",
    "href": "demos/04_railway_example.html",
    "title": "ninjalabo.llmcam",
    "section": "",
    "text": "import json\nimport openai\nimport requests\n\nfrom llmcam.core.oas_to_requests import generate_request, toolbox_schema\nfrom llmcam.core.fn_to_fc import *\n\n\n\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n    (\"user\", \"Hello, what can you provide me about railway and train information?\"),\n])\ncomplete(messages, tool_schema)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nHello, what can you provide me about railway and train information?\n&gt;&gt; Assistant:\nI can provide you with a variety of railway and train-related information, including:  1. **Train\nand Passenger Information:**    - Currently active passenger information messages.    - Messages\nupdated after a specific date.    - Real-time updates and locations for trains.    - GTFS data for\npassenger and all trains.  2. **Trains and Routes:**    - Information about specific trains by their\nnumber and departure date.    - Recent trains that are newer than a specified version.    - Trains\nrunning on a specific departure date.    - Routesets and composition of trains.  3. **Stations:**\n- List of stations and their geo-locations.    - Trains traveling through or between specific\nstations.  4. **Train Operations:**    - Train tracking messages by train number or station.    -\nNotices related to traffic restrictions and track work.  5. **Metadata and Categories:**    - List\nof train types and categories.    - Detailed cause and third cause category codes.    - Operators\ninformation.    - Time table periods.  Please let me know what specific information you would like,\nand I'll be happy to assist you!\n\n\n\nuser_messages = [\n    \"I want to know the train schedule to Joensuu\",\n    \"I want nearest departure information from Helsinki\",\n    \"Where is that train now?\",\n    \"How about its passenger information?\",\n    \"Is there anything to pay attention to about the traffic situation?\",\n]\nfor msg in user_messages:\n    messages.append(form_msg(\"user\", msg))\n    complete(messages, tool_schema)\n\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nHello, what can you provide me about railway and train information?\n&gt;&gt; Assistant:\nI can provide you with a variety of railway and train-related information, including:  1. **Train\nand Passenger Information:**    - Currently active passenger information messages.    - Messages\nupdated after a specific date.    - Real-time updates and locations for trains.    - GTFS data for\npassenger and all trains.  2. **Trains and Routes:**    - Information about specific trains by their\nnumber and departure date.    - Recent trains that are newer than a specified version.    - Trains\nrunning on a specific departure date.    - Routesets and composition of trains.  3. **Stations:**\n- List of stations and their geo-locations.    - Trains traveling through or between specific\nstations.  4. **Train Operations:**    - Train tracking messages by train number or station.    -\nNotices related to traffic restrictions and track work.  5. **Metadata and Categories:**    - List\nof train types and categories.    - Detailed cause and third cause category codes.    - Operators\ninformation.    - Time table periods.  Please let me know what specific information you would like,\nand I'll be happy to assist you!\n&gt;&gt; User:\nI want to know the train schedule to Joensuu\n&gt;&gt; Assistant:\nTo provide you with the train schedule to Joensuu, please let me know your departure station and the\ndate of travel. This will help in fetching the most relevant train schedule information for you.\n&gt;&gt; User:\nI want nearest departure information from Helsinki\n&gt;&gt; Assistant:\nHere are the upcoming train departures from Helsinki to Joensuu:  ### Train 5 (IC - Long-distance) -\n**Departure from Helsinki (Track 7):** November 16, 2024, at 14:19 - **Arrival at Joensuu (Track\n1):** November 16, 2024, at 17:51  ### Train 11 (IC - Long-distance) - **Departure from Helsinki\n(Track 9):** November 16, 2024, at 16:25 - **Arrival at Joensuu (Track 1):** November 16, 2024, at\n21:15  ### Train 1 (IC - Long-distance) - **Departure from Helsinki (Track 10):** November 17, 2024,\nat 04:54 - **Arrival at Joensuu (Track 1):** November 17, 2024, at 09:40  ### Train 3 (IC - Long-\ndistance) - **Departure from Helsinki (Track 9):** November 17, 2024, at 08:19 - **Arrival at\nJoensuu (Track 1):** November 17, 2024, at 12:51  These trains are operated by VR and are\ncategorized as long-distance InterCity (IC) services. Please check with the operator for any changes\nor updates closer to your travel date.\n&gt;&gt; User:\nWhere is that train now?\n&gt;&gt; Assistant:\nHere's the current status of the trains to Joensuu:  - **Train 3** is currently running:   -\n**Location:** Latitude 61.276187, Longitude 29.01863   - **Speed:** 134 km/h   - **Accuracy:** 371\nmeters  The other trains (Train 5, Train 11, and Train 1) currently do not have location data\navailable. They might not have started their journey yet or might be inactive at the moment.\n&gt;&gt; User:\nHow about its passenger information?\n&gt;&gt; Assistant:\nCurrently, there are no active passenger information messages available for trains 5, 11, 1, and 3\nheading to Joensuu. Everything might be running smoothly without any significant updates or alerts\nfor these trains. Let me know if you need any other information!\n&gt;&gt; User:\nIs there anything to pay attention to about the traffic situation?\n&gt;&gt; Assistant:\nThere are several traffic restrictions currently reported, but I don't have specific details on each\none. If you want to know more about a specific restriction, I can provide detailed information.\nGenerally, it's a good idea to be aware of any potential restrictions or planned disruptions when\ntraveling by train, especially over long distances.  If you need more specific details on any\ntraffic restrictions, please let me know, and I can delve into the individual notifications for you!"
  },
  {
    "objectID": "demos/demo_scenario_0.html",
    "href": "demos/demo_scenario_0.html",
    "title": "Demo Scenario 0",
    "section": "",
    "text": "The goal of this demo is to demonstrate the scalability and adaptability of GPT Function calling framework.\nThe scenario of this demo is a user wanting to build an application / service with custom functions. In a normal development flow, the user must start with the primitive utilities, then manually code and link each function together to ensure they work in sequence. This can be a tedious process, especially as the complexity of the application grows and third-party services are involved.\nWith our application, the process is simplified. The GPT Function Calling framework allows the user to easily chain and combine custom functions through simple, natural language instructions. Instead of manually coding each function’s integration, the user can focus on defining high-level tasks, while GPT FC handles the technical complexities of connecting, sequencing, and executing the functions. Additionally, our llmcam.core utilities also help streamline the process by providing a set of pre-built functions that prepare and execute GPT FC.\nIn this case, suppose the user wants to create a service that capture images from Youtube with the provided locations and providers.",
    "crumbs": [
      "Demos",
      "Demo Scenario 0"
    ]
  },
  {
    "objectID": "demos/demo_scenario_0.html#step-1---preparation",
    "href": "demos/demo_scenario_0.html#step-1---preparation",
    "title": "Demo Scenario 0",
    "section": "Step 1 - Preparation",
    "text": "Step 1 - Preparation\nFirst, we define the primitive Python function that handles a specific task - to provide a Youtube Live URL based on location and provider, and build it to a GPT-compatible tool.\nThe main tasks in this step include:\n\nImport necessary modules from llmcam.core.\n\nDefine the primitive function.\n\nAppropriately document the primitive function.\n\nGenerate the tool schema from function.\n\n(Optional) Edit tool schema.\n\n\n# Import necessary modules from `llmcam.core` package\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import *\n\n\n# Define the function for selecting Youtube Live URL\ndef select_youtube_live_url(location, provider):\n    if \"santa\" in location.lower():\n        return \"https://www.youtube.com/watch?v=Cp4RRAEgpeU\"\n    if \"parking\" in location.lower():\n        return \"https://www.youtube.com/watch?v=mwN6l3O1MNI\"\n    else:\n        return \"https://www.youtube.com/watch?v=LMZQ7eFhm58\"\n\n\n# Add documentation to the function:\n    # - Add an overall description of the function\n    # - Add type hints for the input parameters\n    # - Add descriptions for the input parameters (inline comments)\nfrom typing import Optional\n\ndef select_youtube_live_url(\n    location: str,  # Location of the Youtube Live URL\n    provider: Optional[str] = \"Youtube\"  # Provider of the Youtube Live URL\n) -&gt; str:\n    \"\"\"Select Youtube Live URL based on the location and provider.\"\"\"\n    if \"santa\" in location.lower():\n        return \"https://www.youtube.com/watch?v=Cp4RRAEgpeU\"\n    if \"parking\" in location.lower():\n        return \"https://www.youtube.com/watch?v=mwN6l3O1MNI\"\n    else:\n        return \"https://www.youtube.com/watch?v=LMZQ7eFhm58\"\n\n\n# Create a schema for the function\nschema = function_schema(select_youtube_live_url, \"youtube_live\")\nschema\n\n{'type': 'function',\n 'function': {'name': 'select_youtube_live_url',\n  'description': 'Select Youtube Live URL based on the location and provider.',\n  'parameters': {'type': 'object',\n   'properties': {'location': {'type': 'string',\n     'description': 'Location of the Youtube Live URL'},\n    'provider': {'anyOf': [{'type': 'string',\n       'description': 'Provider of the Youtube Live URL'},\n      {'type': 'null',\n       'description': 'A default value will be automatically used.'}]}},\n   'required': ['location']},\n  'metadata': {'module': '__main__', 'service': 'youtube_live'}}}\n\n\n\n# Edit the schema to add more details\nschema[\"function\"][\"parameters\"][\"properties\"][\"location\"][\"description\"] = \"Location of the Youtube Live URL. \\\nPossible values are 'Santa' for Santa Claus Village, 'Parking' for Parking lots, or any other location.\"",
    "crumbs": [
      "Demos",
      "Demo Scenario 0"
    ]
  },
  {
    "objectID": "demos/demo_scenario_0.html#step-2---integrate-function-into-gpt-function-calling",
    "href": "demos/demo_scenario_0.html#step-2---integrate-function-into-gpt-function-calling",
    "title": "Demo Scenario 0",
    "section": "Step 2 - Integrate function into GPT Function calling",
    "text": "Step 2 - Integrate function into GPT Function calling\nWe can start integrating this function into GPT FC for our application. This step involves:\n\nSet up initial tools list and add this function as a built-in tool.\n\nVerify by asking about available tools.\n\nVerify by executing the function.\n\n\n# Set up `tools` list and add `schema` to it\ntools = []\ntools.append(schema)\n\n\n# Start the conversation and verify the tools\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n    (\"user\", \"What tools can you use?\")\n])\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI can use the following tools:  1. **Select Youtube Live URL**: This tool allows you to select a\nYoutube Live URL based on the location (such as 'Santa', 'Parking', or any other location) and\noptionally filter by a specific provider.  2. **Multi Tool Use (Parallel Execution)**: This tool\nallows me to execute multiple functions simultaneously, but only if they can operate in parallel.\nThese tools help in accessing specific Youtube Live URLs based on defined criteria. Let me know how\nI can assist you using these tools!\n\n\n\n# Ask the user for a Youtube Live URL\nmessages.append(form_msg(\"user\", \"Can you provide me with a Youtube Live URL from Santa Claus Village?\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nHere is a Youtube Live URL from Santa Claus Village: [Watch\nLive](https://www.youtube.com/watch?v=Cp4RRAEgpeU). Enjoy the view!",
    "crumbs": [
      "Demos",
      "Demo Scenario 0"
    ]
  },
  {
    "objectID": "demos/demo_scenario_0.html#step-3---integrate-with-any-other-tools",
    "href": "demos/demo_scenario_0.html#step-3---integrate-with-any-other-tools",
    "title": "Demo Scenario 0",
    "section": "Step 3 - Integrate with any other tools",
    "text": "Step 3 - Integrate with any other tools\nTo demonstrate the scalability of this system, we can extend the tools list by some of llmcam.vision built-in functions. This step include:\n\nImport function to capture image from a Youtube Live llmcam.vision.ytlive.\n\nGenerate and add its schema to the tools list.\n\nTest integration and chain the primitive functions with a command to capture image from some locations.\n\n\nfrom llmcam.vision.ytlive import capture_youtube_live_frame\n\n\ntools.append(function_schema(capture_youtube_live_frame, \"youtube_live\"))\nlen(tools)\n\n2\n\n\n\n# Ask GPT to capture a frame from the a known location\nmessages.append(form_msg(\n    \"user\", \n    \"Can you capture a frame from the Youtube Live URL of Santa Claus Village? \\\nTell me the file path of the captured frame.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading mweb player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n&gt;&gt; Assistant:\nI have captured a frame from the Youtube Live stream of Santa Claus Village. The image has been\nsaved at the following file path:\n`/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_05:54:54_Santa.jpg`.\n\n\n\n# Display the captured image\nfrom IPython.display import Image\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_05:54:54_Santa.jpg\")\n\n\n\n\n\n\n\n\nAs such, GPT FC has successfully built a higher-level function of capturing an image from a location from two primitive functions. For examples of constructing built-in functions or more complex frameworks, refer to our modules in llmcam.vision and llmcam.utils. For a full application, test our chatbot website on http://llmcam.ninjalabo.ai/ or run it locally with llmcam.application.runner.run_llmcam.",
    "crumbs": [
      "Demos",
      "Demo Scenario 0"
    ]
  },
  {
    "objectID": "demos/demo_scenario_0.html#conversation-transcript",
    "href": "demos/demo_scenario_0.html#conversation-transcript",
    "title": "Demo Scenario 0",
    "section": "CONVERSATION TRANSCRIPT",
    "text": "CONVERSATION TRANSCRIPT\n\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nWhat tools can you use?\n&gt;&gt; Assistant:\nI can use the following tools:  1. **Select Youtube Live URL**: This tool allows you to select a\nYoutube Live URL based on the location (such as 'Santa', 'Parking', or any other location) and\noptionally filter by a specific provider.  2. **Multi Tool Use (Parallel Execution)**: This tool\nallows me to execute multiple functions simultaneously, but only if they can operate in parallel.\nThese tools help in accessing specific Youtube Live URLs based on defined criteria. Let me know how\nI can assist you using these tools!\n&gt;&gt; User:\nCan you provide me with a Youtube Live URL from Santa Claus Village?\n&gt;&gt; Assistant:\nHere is a Youtube Live URL from Santa Claus Village: [Watch\nLive](https://www.youtube.com/watch?v=Cp4RRAEgpeU). Enjoy the view!\n&gt;&gt; User:\nCan you capture a frame from the Youtube Live URL of Santa Claus Village? Tell me the file path of\nthe captured frame.\n&gt;&gt; Assistant:\nI have captured a frame from the Youtube Live stream of Santa Claus Village. The image has been\nsaved at the following file path:\n`/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_05:54:54_Santa.jpg`.",
    "crumbs": [
      "Demos",
      "Demo Scenario 0"
    ]
  },
  {
    "objectID": "demos/demo_scenario_2.html",
    "href": "demos/demo_scenario_2.html",
    "title": "Demo Scenario 2",
    "section": "",
    "text": "The goal of this scenario is to demonstrate our built-in tools for integrating computer vision to GPT Function calling in a relevant use case.\nThe scenario of this demo is a user wanting to test different vision models and visualize their outputs through plotting. Without our application, the user would need to manually set up and configure the models. This process can be complex, requiring significant technical expertise, time, and effort to test various models, handle compatibility issues, and ensure smooth operation. This manual approach also lacks scalability and replicability, as the workflow need to be re-implemented for different systems or teams.\nWith our application, this entire process is simplified. Our built-in tools allow the user to specify tasks in plain language, and GPT FC handles the setup, selection, and coordination of the appropriate models and reporting tools. Additionally, GPT FC enables real-time interactions and allows flexible choices in building Machine learning pipelines. It can also dynamically imports and integrates new tools, adapting easily to evolving user needs.\nFor this demo, we will be using the Youtube Live feed https://www.youtube.com/watch?v=Cp4RRAEgpeU at Santa Claus Village.",
    "crumbs": [
      "Demos",
      "Demo Scenario 2"
    ]
  },
  {
    "objectID": "demos/demo_scenario_2.html#step-1---built-in-llmcam.vision-modules",
    "href": "demos/demo_scenario_2.html#step-1---built-in-llmcam.vision-modules",
    "title": "Demo Scenario 2",
    "section": "Step 1 - Built-in llmcam.vision modules",
    "text": "Step 1 - Built-in llmcam.vision modules\nFor this demo, we will add some default built-in tools from llmcam.vision. At this step, the main steps are:\n\nImport llmcam.core modules and llmcam.vision modules.\n\nSet up initial tools list and add built-in tools from llmcam.vision.\n\nVerify set up with GPT messages.\n\n\n# Import all the base modules and vision modules\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import *\nfrom llmcam.vision.ytlive import *\nfrom llmcam.vision.gpt4v import *\nfrom llmcam.vision.yolo import *\nfrom llmcam.vision.plotting import *\n\n\n# Define the `tools` list with default built-in functions\ntools = [function_schema(func, \"llmcam_vision\") for func in [\n    capture_youtube_live_frame, \n    ask_gpt4v_about_image_file,\n    detect_objects,\n    plot_object\n]]\n\n\n# Start the conversation and verify the tools\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n    (\"user\", \"What tools can you use?\")\n])\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI can use the following tools:  1. **capture_youtube_live_frame**: Capture a jpeg image from a\nYouTube Live stream and return the path to the saved image.  2. **ask_gpt4v_about_image_file**:\nProvide quantitative information about a given image file.  3. **detect_objects**: Detect objects in\nan image using the YOLO model.  4. **plot_object**: Generate a bar plot displaying the number of\ninstances of a specified object detected in a list of images, using specified methods (\"gpt\" and\n\"yolo\").",
    "crumbs": [
      "Demos",
      "Demo Scenario 2"
    ]
  },
  {
    "objectID": "demos/demo_scenario_2.html#step-2---test-different-vision-models",
    "href": "demos/demo_scenario_2.html#step-2---test-different-vision-models",
    "title": "Demo Scenario 2",
    "section": "Step 2 - Test different vision models",
    "text": "Step 2 - Test different vision models\nOur built-in tools contain 2 models - YOLO object detection and GPT model. Both these tools only need the image as their inputs. This image can be retrieved from the Youtube Live with another built-in tool.\nAt this step, the main tasks include:\n\nCapture an image from Santa Clause Village with its link: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n\nUse GPT model to detect the number of people and basic information.\n\nUse YOLO to detect the number of people and any other objects.\n\n\n# Capture the live feed from Santa Claus Village\nmessages.append(form_msg(\n    \"user\", \n    \"Here is the live feed for Santa Claus Village https://www.youtube.com/watch?v=Cp4RRAEgpeU to capture. \\\nTell me also the file path of the saved image.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading mweb player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n&gt;&gt; Assistant:\nThe image from the Santa Claus Village live feed has been captured and saved at the following file\npath: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:07_None.jpg`.\n\n\n\n# Display the captured image\nfrom IPython.display import Image\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:07_None.jpg\")\n\n\n\n\n\n\n\n\n\n# Ask the GPT-4 Vision model about the image\nmessages.append(form_msg(\n    \"user\", \n    \"Use the GPT-4 Vision model to tell the number of people and briefly describe the image.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nThe image captured from the Santa Claus Village live feed shows:  - **Number of People**:\nApproximately 50 people are present. - **Description**: The scene takes place at night with\nprominent artificial lighting illuminating the area. There are 3 buildings visible, ranging from 1\nto 2 stories in height. The atmosphere appears lively, possibly with people gathered for an event or\nsimply enjoying the winter ambiance. Additionally, there are 5 trees adorned with lights, adding to\nthe festive atmosphere. The temperature is around -10°C, and the sky is not visible in the image.\n\n\n\n# Detect the objects in the image using YOLO\nmessages.append(form_msg(\n    \"user\",\n    \"Use YOLO to detect the number of people in the image and any other objects \\\nin the image.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:07_None.jpg: 384x640 16 persons, 72.2ms\nSpeed: 7.0ms preprocess, 72.2ms inference, 223.1ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; Assistant:\nUsing the YOLO model, the image contains:  - **Number of People**: 16  No other objects were\ndetected in the image besides people.\n\n\n\n# Display the detection image\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/detection_cap_2024.12.20_16:12:07_None.jpg\")",
    "crumbs": [
      "Demos",
      "Demo Scenario 2"
    ]
  },
  {
    "objectID": "demos/demo_scenario_2.html#step-3---pipeline-extension-with-plotting",
    "href": "demos/demo_scenario_2.html#step-3---pipeline-extension-with-plotting",
    "title": "Demo Scenario 2",
    "section": "Step 3 - Pipeline extension with plotting",
    "text": "Step 3 - Pipeline extension with plotting\nWe can further extend the comparison pipeline with any reporting tools or monitoring utilities (demonstrated in demo scenario 3). In this demo, we will use the built-in tool from llmcam.vision.plotting to create bar plot between these two models.\nAt this step, the main tasks include:\n\nCapture another image to increase data volume in plots.\n\nPlot the number of people with both models.\n\n\n# Capture another image from the live feed\nmessages.append(form_msg(\n    \"user\", \n    \"Capture another image from the live feed.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading mweb player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n&gt;&gt; Assistant:\nAnother image from the Santa Claus Village live feed has been captured and saved at the following\nfile path: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:23_None.jpg`.\n\n\n\n# Plot the number of people in the captured images using both GPT-4 Vision and YOLO\nmessages.append(form_msg(\n    \"user\",\n    \"Plot the number of people in these captured images using both methods \\\n - GPT retrieval of basic information and YOLO object detection. \\\nTell me the file path of the generated plot.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:07_None.jpg: 384x640 16 persons, 10.4ms\nSpeed: 2.2ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:23_None.jpg: 384x640 19 persons, 10.1ms\nSpeed: 1.2ms preprocess, 10.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; Assistant:\nThe plot displaying the number of people detected in the captured images using both the GPT-4 Vision\nmethod and YOLO object detection has been generated. You can find the plot at the following file\npath: `/home/nghivo/tinyMLaaS/llmcam/data/484_object_count_plot.jpg`.\n\n\n\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/484_object_count_plot.jpg\")",
    "crumbs": [
      "Demos",
      "Demo Scenario 2"
    ]
  },
  {
    "objectID": "demos/demo_scenario_2.html#conversation-transcript",
    "href": "demos/demo_scenario_2.html#conversation-transcript",
    "title": "Demo Scenario 2",
    "section": "CONVERSATION TRANSCRIPT",
    "text": "CONVERSATION TRANSCRIPT\n\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nWhat tools can you use?\n&gt;&gt; Assistant:\nI can use the following tools:  1. **capture_youtube_live_frame**: Capture a jpeg image from a\nYouTube Live stream and return the path to the saved image.  2. **ask_gpt4v_about_image_file**:\nProvide quantitative information about a given image file.  3. **detect_objects**: Detect objects in\nan image using the YOLO model.  4. **plot_object**: Generate a bar plot displaying the number of\ninstances of a specified object detected in a list of images, using specified methods (\"gpt\" and\n\"yolo\").\n&gt;&gt; User:\nHere is the live feed for Santa Claus Village https://www.youtube.com/watch?v=Cp4RRAEgpeU to\ncapture. Tell me also the file path of the saved image.\n&gt;&gt; Assistant:\nThe image from the Santa Claus Village live feed has been captured and saved at the following file\npath: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:07_None.jpg`.\n&gt;&gt; User:\nUse the GPT-4 Vision model to tell the number of people and briefly describe the image.\n&gt;&gt; Assistant:\nThe image captured from the Santa Claus Village live feed shows:  - **Number of People**:\nApproximately 50 people are present. - **Description**: The scene takes place at night with\nprominent artificial lighting illuminating the area. There are 3 buildings visible, ranging from 1\nto 2 stories in height. The atmosphere appears lively, possibly with people gathered for an event or\nsimply enjoying the winter ambiance. Additionally, there are 5 trees adorned with lights, adding to\nthe festive atmosphere. The temperature is around -10°C, and the sky is not visible in the image.\n&gt;&gt; User:\nUse YOLO to detect the number of people in the image and any other objects in the image.\n&gt;&gt; Assistant:\nUsing the YOLO model, the image contains:  - **Number of People**: 16  No other objects were\ndetected in the image besides people.\n&gt;&gt; User:\nCapture another image from the live feed.\n&gt;&gt; Assistant:\nAnother image from the Santa Claus Village live feed has been captured and saved at the following\nfile path: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_16:12:23_None.jpg`.\n&gt;&gt; User:\nPlot the number of people in these captured images using both methods  - GPT retrieval of basic\ninformation and YOLO object detection. Tell me the file path of the generated plot.\n&gt;&gt; Assistant:\nThe plot displaying the number of people detected in the captured images using both the GPT-4 Vision\nmethod and YOLO object detection has been generated. You can find the plot at the following file\npath: `/home/nghivo/tinyMLaaS/llmcam/data/484_object_count_plot.jpg`.",
    "crumbs": [
      "Demos",
      "Demo Scenario 2"
    ]
  },
  {
    "objectID": "vision/plotting.html",
    "href": "vision/plotting.html",
    "title": "Plots from GPT",
    "section": "",
    "text": "From the current modules in vision, we can retrieve information about an image. These information includes the counts of objects per class. Therefore, we can plot a bar chart for counts of objects of one class throughout the image list.\nFor this example, let us use captured images from Santa Claus Village to plot the number of people.\n\nfrom pathlib import Path\nimport glob\n\ndata_path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\")).absolute()\nfiles = sorted(glob.glob(f\"{data_path}/cap_*.jpg\"))\nfiles = [ file for file in files if \"santaclausvillage\" in file ]\nfiles = files[:2]\n\n\nfrom IPython.display import display, Image\ndisplay(Image(files[0]))\n\n\n\n\n\n\n\n\n\nfrom IPython.display import display, Image\ndisplay(Image(files[1]))\n\n\n\n\n\n\n\n\n\n# Define object to count\nobj = \"person\"\n\n\n\n\n# Initiate counts per image\ncount = []\n\n# Count objects in images\nfor image in files:\n    info = json.loads(ask_gpt4v_about_image_file(image))\n    count.append(info.get(obj, 0))\n\ncount\n\n[50, 50]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(files, count, color='skyblue')\nplt.title(f'Number of {obj} Detected per Image')\nplt.xlabel('Image')\nplt.ylabel(f'Number of {obj}')\nplt.xticks(range(len(files)), [f\"Image {i+1}\" for i in range(len(files))], rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Initiate counts per image\ncount = []\n\n# Count objects in images\nfor image in files:\n    info = json.loads(detect_objects(image))\n    count.append(info.get(obj, 0))\n\ncount\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_13:59:40_santaclausvillage.jpg: 384x640 21 persons, 2 handbags, 74.3ms\nSpeed: 2.0ms preprocess, 74.3ms inference, 582.3ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_14:02:23_santaclausvillage.jpg: 384x640 21 persons, 1 backpack, 3 handbags, 18.2ms\nSpeed: 2.1ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n\n[21, 21]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(files, count, color='skyblue')\nplt.title(f'Number of {obj} Detected per Image')\nplt.xlabel('Image')\nplt.ylabel(f'Number of {obj}')\nplt.xticks(range(len(files)), [f\"Image {i+1}\" for i in range(len(files))], rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can combine these plots with pyplot.subplots and summarize this whole process into a single function to be integrated into our GPT Function calling framework:\n\nsource\n\n\n\n\n plot_object (images:list[str], object:str, methods:list[str]=['gpt',\n              'yolo'])\n\nGenerate (only when requested) a bar plot displaying the number of instances of a specified object detected in a list of images, accepting only objects in singular form. Change the methods name to lowercase before passing to the function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimages\nlist\n\nList of images to be extracted\n\n\nobject\nstr\n\nObject to detect\n\n\nmethods\nlist\n[‘gpt’, ‘yolo’]\nList of methods to use for extracting information. The available methods are “gpt” and “yolo”\n\n\n\n\nres = plot_object(files, \"person\", [\"gpt\", \"yolo\"])\npath = json.loads(res).get(\"path\")\ndisplay(Image(path))\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_13:59:40_santaclausvillage.jpg: 384x640 21 persons, 2 handbags, 9.5ms\nSpeed: 1.7ms preprocess, 9.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_14:02:23_santaclausvillage.jpg: 384x640 21 persons, 1 backpack, 3 handbags, 8.8ms\nSpeed: 1.7ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)",
    "crumbs": [
      "Vision",
      "Plots from GPT"
    ]
  },
  {
    "objectID": "vision/plotting.html#generating-plots-from-yolo-and-gpt-vision-models",
    "href": "vision/plotting.html#generating-plots-from-yolo-and-gpt-vision-models",
    "title": "Plots from GPT",
    "section": "",
    "text": "From the current modules in vision, we can retrieve information about an image. These information includes the counts of objects per class. Therefore, we can plot a bar chart for counts of objects of one class throughout the image list.\nFor this example, let us use captured images from Santa Claus Village to plot the number of people.\n\nfrom pathlib import Path\nimport glob\n\ndata_path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\")).absolute()\nfiles = sorted(glob.glob(f\"{data_path}/cap_*.jpg\"))\nfiles = [ file for file in files if \"santaclausvillage\" in file ]\nfiles = files[:2]\n\n\nfrom IPython.display import display, Image\ndisplay(Image(files[0]))\n\n\n\n\n\n\n\n\n\nfrom IPython.display import display, Image\ndisplay(Image(files[1]))\n\n\n\n\n\n\n\n\n\n# Define object to count\nobj = \"person\"\n\n\n\n\n# Initiate counts per image\ncount = []\n\n# Count objects in images\nfor image in files:\n    info = json.loads(ask_gpt4v_about_image_file(image))\n    count.append(info.get(obj, 0))\n\ncount\n\n[50, 50]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(files, count, color='skyblue')\nplt.title(f'Number of {obj} Detected per Image')\nplt.xlabel('Image')\nplt.ylabel(f'Number of {obj}')\nplt.xticks(range(len(files)), [f\"Image {i+1}\" for i in range(len(files))], rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n# Initiate counts per image\ncount = []\n\n# Count objects in images\nfor image in files:\n    info = json.loads(detect_objects(image))\n    count.append(info.get(obj, 0))\n\ncount\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_13:59:40_santaclausvillage.jpg: 384x640 21 persons, 2 handbags, 74.3ms\nSpeed: 2.0ms preprocess, 74.3ms inference, 582.3ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_14:02:23_santaclausvillage.jpg: 384x640 21 persons, 1 backpack, 3 handbags, 18.2ms\nSpeed: 2.1ms preprocess, 18.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n\n\n[21, 21]\n\n\n\nplt.figure(figsize=(10, 6))\nplt.bar(files, count, color='skyblue')\nplt.title(f'Number of {obj} Detected per Image')\nplt.xlabel('Image')\nplt.ylabel(f'Number of {obj}')\nplt.xticks(range(len(files)), [f\"Image {i+1}\" for i in range(len(files))], rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can combine these plots with pyplot.subplots and summarize this whole process into a single function to be integrated into our GPT Function calling framework:\n\nsource\n\n\n\n\n plot_object (images:list[str], object:str, methods:list[str]=['gpt',\n              'yolo'])\n\nGenerate (only when requested) a bar plot displaying the number of instances of a specified object detected in a list of images, accepting only objects in singular form. Change the methods name to lowercase before passing to the function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimages\nlist\n\nList of images to be extracted\n\n\nobject\nstr\n\nObject to detect\n\n\nmethods\nlist\n[‘gpt’, ‘yolo’]\nList of methods to use for extracting information. The available methods are “gpt” and “yolo”\n\n\n\n\nres = plot_object(files, \"person\", [\"gpt\", \"yolo\"])\npath = json.loads(res).get(\"path\")\ndisplay(Image(path))\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_13:59:40_santaclausvillage.jpg: 384x640 21 persons, 2 handbags, 9.5ms\nSpeed: 1.7ms preprocess, 9.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_14:02:23_santaclausvillage.jpg: 384x640 21 persons, 1 backpack, 3 handbags, 8.8ms\nSpeed: 1.7ms preprocess, 8.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)",
    "crumbs": [
      "Vision",
      "Plots from GPT"
    ]
  },
  {
    "objectID": "vision/plotting.html#simulated-gpt-framework",
    "href": "vision/plotting.html#simulated-gpt-framework",
    "title": "Plots from GPT",
    "section": "Simulated GPT framework",
    "text": "Simulated GPT framework\nThis section tests integrating with our current GPT framework. This function can be used in combination with previous functions in the vision module and file_manager.list_image_files:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\nfrom llmcam.utils.file_manager import list_image_files\n\ntools = [\n    function_schema(list_image_files, \"Local files\"),\n    function_schema(plot_object, \"Plot objects\"),\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"List local image files from Santa Claus Village.\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nList local image files from Santa Claus Village.\n&gt;&gt; Assistant:\nHere are the local image files from Santa Claus Village:  1.\ncap_2025.01.16_18:25:30_santaclausvillage.jpg 2. cap_2025.01.14_02:33:41_santaclausvillage.jpg 3.\ncap_2025.01.14_01:52:14_santaclausvillage.jpg 4. cap_2024.12.11_14:02:23_santaclausvillage.jpg 5.\ncap_2024.12.11_13:59:40_santaclausvillage.jpg 6. cap_2024.12.09_20:33:06_santaclausvillege.jpg 7.\ncap_2024.12.09_20:32:50_santaclausvillege.jpg 8. cap_2025.01.14_02:14:33_santaclausvillage.jpg  Let\nme know if you need further assistance!\n\n\n\n# Continue the conversation and plot using both methods\nmessages.append(form_msg(\"user\", \"Plot the number of people detected in the images using both YOLO and GPT-4 Vision. Tell me the path to the plot.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:25:30_santaclausvillage.jpg: 384x640 20 persons, 1 handbag, 35.1ms\nSpeed: 3.0ms preprocess, 35.1ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.14_02:33:41_santaclausvillage.jpg: 384x640 (no detections), 28.0ms\nSpeed: 2.4ms preprocess, 28.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.14_01:52:14_santaclausvillage.jpg: 384x640 1 person, 11.7ms\nSpeed: 2.0ms preprocess, 11.7ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_14:02:23_santaclausvillage.jpg: 384x640 21 persons, 1 backpack, 3 handbags, 10.6ms\nSpeed: 1.4ms preprocess, 10.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.11_13:59:40_santaclausvillage.jpg: 384x640 21 persons, 2 handbags, 9.7ms\nSpeed: 1.5ms preprocess, 9.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:33:06_santaclausvillege.jpg: 384x640 12 persons, 25.9ms\nSpeed: 1.7ms preprocess, 25.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:32:50_santaclausvillege.jpg: 384x640 9 persons, 1 potted plant, 13.1ms\nSpeed: 1.8ms preprocess, 13.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.14_02:14:33_santaclausvillage.jpg: 384x640 (no detections), 11.6ms\nSpeed: 1.8ms preprocess, 11.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nList local image files from Santa Claus Village.\n&gt;&gt; Assistant:\nHere are the local image files from Santa Claus Village:  1.\ncap_2025.01.16_18:25:30_santaclausvillage.jpg 2. cap_2025.01.14_02:33:41_santaclausvillage.jpg 3.\ncap_2025.01.14_01:52:14_santaclausvillage.jpg 4. cap_2024.12.11_14:02:23_santaclausvillage.jpg 5.\ncap_2024.12.11_13:59:40_santaclausvillage.jpg 6. cap_2024.12.09_20:33:06_santaclausvillege.jpg 7.\ncap_2024.12.09_20:32:50_santaclausvillege.jpg 8. cap_2025.01.14_02:14:33_santaclausvillage.jpg  Let\nme know if you need further assistance!\n&gt;&gt; User:\nPlot the number of people detected in the images using both YOLO and GPT-4 Vision. Tell me the path\nto the plot.\n&gt;&gt; Assistant:\nThe plot showing the number of people detected in the images using both YOLO and GPT-4 Vision has\nbeen generated. You can find the plot at the following path:\n`/home/nghivo/tinyMLaaS/llmcam/data/667_object_count_plot.jpg`\n\n\n\ndisplay(Image('/home/nghivo/tinyMLaaS/llmcam/data/667_object_count_plot.jpg'))",
    "crumbs": [
      "Vision",
      "Plots from GPT"
    ]
  },
  {
    "objectID": "vision/yolo.html",
    "href": "vision/yolo.html",
    "title": "YOLO",
    "section": "",
    "text": "Apart from processing images with GPT vision model, we can also use a YOLO model to detect objects in images. This section demonstrate this process, starting with a captured image of Santa Claus Village:\n\nimport os\nfrom IPython.display import Image\n\ndata_path = os.getenv(\"LLMCAM_DATA\", \"../data\")\nfile = f\"{data_path}/cap_2024.12.09_20:33:06_santaclausvillege.jpg\"\nImage(file)\n\n\n\n\n\n\n\n\nDetect objects as bounding boxes with a YOLO model:\n\n# Load pretrained YOLO model\nmodel = YOLO('yolov8s.pt')\n\n# Get detections\nresult = model(file, conf=0.25, exist_ok=True)[0]\n\n# Save results as image\nsave_dir = os.getenv(\"LLMCAM_DATA\", \"../data\")\noutput_path = file.split(\"/\")[-1]\nresult.save(filename=f\"{save_dir}/detection_{output_path}\")\n\n# Display results\nImage(f\"{save_dir}/detection_{output_path}\")\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:33:06_santaclausvillege.jpg: 384x640 12 persons, 13.4ms\nSpeed: 1.3ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n\n\n\n\n\n\n\n\nDetected objects can be summarized with counts of objects per class. This summarized information is useful as tool results for our GPT Function calling framework.\n\n# Count objects per class\ndict = {}\nfor c in result.boxes.cls:\n    dict[model.names[int(c)]] = dict.get(model.names[int(c)], 0) + 1\njson.dumps(dict)\n\n'{\"person\": 12}'\n\n\nModularized operations:\n\nsource\n\n\n\n detect_objects (image_path:str, conf:Optional[float]=0.25)\n\nDetect object in the input image using YOLO model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_path\nstr\n\nPath/URL of image\n\n\nconf\nOptional\n0.25\nConfidence threshold\n\n\nReturns\nstr\n\nJSON format of detection results\n\n\n\nTest usage:\n\ndetect_objects(file, 0.25)\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:33:06_santaclausvillege.jpg: 384x640 12 persons, 12.4ms\nSpeed: 1.7ms preprocess, 12.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n\n'{\"person\": 12}'",
    "crumbs": [
      "Vision",
      "YOLO"
    ]
  },
  {
    "objectID": "vision/yolo.html#detecting-objects-with-yolo",
    "href": "vision/yolo.html#detecting-objects-with-yolo",
    "title": "YOLO",
    "section": "",
    "text": "Apart from processing images with GPT vision model, we can also use a YOLO model to detect objects in images. This section demonstrate this process, starting with a captured image of Santa Claus Village:\n\nimport os\nfrom IPython.display import Image\n\ndata_path = os.getenv(\"LLMCAM_DATA\", \"../data\")\nfile = f\"{data_path}/cap_2024.12.09_20:33:06_santaclausvillege.jpg\"\nImage(file)\n\n\n\n\n\n\n\n\nDetect objects as bounding boxes with a YOLO model:\n\n# Load pretrained YOLO model\nmodel = YOLO('yolov8s.pt')\n\n# Get detections\nresult = model(file, conf=0.25, exist_ok=True)[0]\n\n# Save results as image\nsave_dir = os.getenv(\"LLMCAM_DATA\", \"../data\")\noutput_path = file.split(\"/\")[-1]\nresult.save(filename=f\"{save_dir}/detection_{output_path}\")\n\n# Display results\nImage(f\"{save_dir}/detection_{output_path}\")\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:33:06_santaclausvillege.jpg: 384x640 12 persons, 13.4ms\nSpeed: 1.3ms preprocess, 13.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n\n\n\n\n\n\n\n\n\nDetected objects can be summarized with counts of objects per class. This summarized information is useful as tool results for our GPT Function calling framework.\n\n# Count objects per class\ndict = {}\nfor c in result.boxes.cls:\n    dict[model.names[int(c)]] = dict.get(model.names[int(c)], 0) + 1\njson.dumps(dict)\n\n'{\"person\": 12}'\n\n\nModularized operations:\n\nsource\n\n\n\n detect_objects (image_path:str, conf:Optional[float]=0.25)\n\nDetect object in the input image using YOLO model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimage_path\nstr\n\nPath/URL of image\n\n\nconf\nOptional\n0.25\nConfidence threshold\n\n\nReturns\nstr\n\nJSON format of detection results\n\n\n\nTest usage:\n\ndetect_objects(file, 0.25)\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.09_20:33:06_santaclausvillege.jpg: 384x640 12 persons, 12.4ms\nSpeed: 1.7ms preprocess, 12.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n\n\n'{\"person\": 12}'",
    "crumbs": [
      "Vision",
      "YOLO"
    ]
  },
  {
    "objectID": "vision/yolo.html#simulated-gpt-workflow",
    "href": "vision/yolo.html#simulated-gpt-workflow",
    "title": "YOLO",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nThis section tests integrating with our current GPT framework. This function can be used in combination with previous functions in the vision module:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\nfrom llmcam.vision.ytlive import capture_youtube_live_frame\nfrom llmcam.vision.gpt4v import ask_gpt4v_about_image_file\n\ntools = [\n    function_schema(capture_youtube_live_frame, \"Youtube Live Capture\"),\n    function_schema(ask_gpt4v_about_image_file, \"GPT4 Vision\"),\n    function_schema(detect_objects, \"YOLO Object Detection\")\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Capture an image from this Youtube Live at Santa Claus Village: https://www.youtube.com/watch?v=Cp4RRAEgpeU\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading mweb player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nCapture an image from this Youtube Live at Santa Claus Village:\nhttps://www.youtube.com/watch?v=Cp4RRAEgpeU\n&gt;&gt; Assistant:\nI have captured an image from the YouTube Live stream at Santa Claus Village. If you need further\nanalysis or have any questions about this image, feel free to ask!\n\n\n\n# Continue the conversation and detect the number of people in the image\nmessages.append(form_msg(\"user\", \"Can you find out how many people there are in this image?\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.12_09:37:48_None.jpg: 384x640 19 persons, 2 backpacks, 48.7ms\nSpeed: 5.4ms preprocess, 48.7ms inference, 189.9ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nCapture an image from this Youtube Live at Santa Claus Village:\nhttps://www.youtube.com/watch?v=Cp4RRAEgpeU\n&gt;&gt; Assistant:\nI have captured an image from the YouTube Live stream at Santa Claus Village. If you need further\nanalysis or have any questions about this image, feel free to ask!\n&gt;&gt; User:\nCan you find out how many people there are in this image?\n&gt;&gt; Assistant:\nThere are 19 people detected in the captured image from the YouTube Live stream at Santa Claus\nVillage. If you have any more questions or need further assistance, feel free to ask!",
    "crumbs": [
      "Vision",
      "YOLO"
    ]
  },
  {
    "objectID": "vision/gpt4v.html",
    "href": "vision/gpt4v.html",
    "title": "Ask GPT-4v",
    "section": "",
    "text": "A simple approach to process image with GPT is to encode image as base64 string and ask about its information. Starting with some local images:\n\nimport os\nfrom pathlib import Path\n\ndata_path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\")).absolute()\nfiles = sorted(glob.glob(f\"{data_path}/cap_*.jpg\"))\nlen(files), files[:9]\n\n(640,\n ['/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_15:59:06_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:00:11_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:01:16_Etelasatama.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:02:21_Etelasatama.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:05:31_Olympiaterminaali.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:06:36_Olympiaterminaali.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:07:41_Torni.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:09:51_Tuomiokirkko.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:15:11_Presidentinlinna.jpg'])\n\n\n\nImage(files[0])\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n encode_image (fname:str)\n\nEncode an image file as base64 string\nExample of encoding:\n\nbase64_image = encode_image(files[0])\nbase64_image[:9]\n\n'/9j/4AAQS'\n\n\nWith the encoded image, we can use OpenAI API to intiate a conversation asking about the image. We can also specify the format of outputs:\n\nresponse = openai.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": question,},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(files[0])}\", \"detail\":\"high\",},\n            },\n        ],\n    }],\n  max_tokens=300,\n)\nresponse.choices[0].message.content\n\n'```json\\n{\\n    \"timestamp\": \"2024-09-28T15:59:06\",\\n    \"location\": \"Presidentinlinna\",\\n    \"dimensions\": \"1280 x 720\",\\n    \"building\": 12,\\n    \"buildings_height_range\": \"3-7 stories\",\\n    \"car\": 30,\\n    \"truck\": 3,\\n    \"boat\": 6,\\n    \"available_parking_space\": 5,\\n    \"street_lights\": 15,\\n    \"person\": 50,\\n    \"time_of_day\": \"afternoon\",\\n    \"artificial_lighting\": \"minimal\",\\n    \"visibility_clear\": true,\\n    \"sky_visible\": true,\\n    \"sky_light_conditions\": \"daylight\",\\n    \"waterbodies_visible\": true,\\n    \"waterbodies_type\": \"harbor\"\\n}\\n```'\n\n\nIt is clear that we need some extra processing steps to convert the JSON-formatted data in strings into Python dictionary.\n\ndef info(response):\n    txt = json.loads(response.json())['choices'][0]['message']['content']\n    data = json.loads(txt.replace('```json\\n', \"\").replace('\\n```', \"\"))\n    return data\n\n\ninfo(response)\n\n/tmp/ipykernel_17475/2561472427.py:3: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n  txt = json.loads(response.json())['choices'][0]['message']['content']\n\n\n{'timestamp': '2024-09-28T15:59:06',\n 'location': 'Presidentinlinna',\n 'dimensions': '1280 x 720',\n 'building': 12,\n 'buildings_height_range': '3-7 stories',\n 'car': 30,\n 'truck': 3,\n 'boat': 6,\n 'available_parking_space': 5,\n 'street_lights': 15,\n 'person': 50,\n 'time_of_day': 'afternoon',\n 'artificial_lighting': 'minimal',\n 'visibility_clear': True,\n 'sky_visible': True,\n 'sky_light_conditions': 'daylight',\n 'waterbodies_visible': True,\n 'waterbodies_type': 'harbor'}",
    "crumbs": [
      "Vision",
      "Ask GPT-4v"
    ]
  },
  {
    "objectID": "vision/gpt4v.html#process-images-with-gpt",
    "href": "vision/gpt4v.html#process-images-with-gpt",
    "title": "Ask GPT-4v",
    "section": "",
    "text": "A simple approach to process image with GPT is to encode image as base64 string and ask about its information. Starting with some local images:\n\nimport os\nfrom pathlib import Path\n\ndata_path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\")).absolute()\nfiles = sorted(glob.glob(f\"{data_path}/cap_*.jpg\"))\nlen(files), files[:9]\n\n(640,\n ['/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_15:59:06_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:00:11_Presidentinlinna.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:01:16_Etelasatama.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:02:21_Etelasatama.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:05:31_Olympiaterminaali.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:06:36_Olympiaterminaali.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:07:41_Torni.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:09:51_Tuomiokirkko.jpg',\n  '/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.09.28_16:15:11_Presidentinlinna.jpg'])\n\n\n\nImage(files[0])\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n encode_image (fname:str)\n\nEncode an image file as base64 string\nExample of encoding:\n\nbase64_image = encode_image(files[0])\nbase64_image[:9]\n\n'/9j/4AAQS'\n\n\nWith the encoded image, we can use OpenAI API to intiate a conversation asking about the image. We can also specify the format of outputs:\n\nresponse = openai.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": question,},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image(files[0])}\", \"detail\":\"high\",},\n            },\n        ],\n    }],\n  max_tokens=300,\n)\nresponse.choices[0].message.content\n\n'```json\\n{\\n    \"timestamp\": \"2024-09-28T15:59:06\",\\n    \"location\": \"Presidentinlinna\",\\n    \"dimensions\": \"1280 x 720\",\\n    \"building\": 12,\\n    \"buildings_height_range\": \"3-7 stories\",\\n    \"car\": 30,\\n    \"truck\": 3,\\n    \"boat\": 6,\\n    \"available_parking_space\": 5,\\n    \"street_lights\": 15,\\n    \"person\": 50,\\n    \"time_of_day\": \"afternoon\",\\n    \"artificial_lighting\": \"minimal\",\\n    \"visibility_clear\": true,\\n    \"sky_visible\": true,\\n    \"sky_light_conditions\": \"daylight\",\\n    \"waterbodies_visible\": true,\\n    \"waterbodies_type\": \"harbor\"\\n}\\n```'\n\n\nIt is clear that we need some extra processing steps to convert the JSON-formatted data in strings into Python dictionary.\n\ndef info(response):\n    txt = json.loads(response.json())['choices'][0]['message']['content']\n    data = json.loads(txt.replace('```json\\n', \"\").replace('\\n```', \"\"))\n    return data\n\n\ninfo(response)\n\n/tmp/ipykernel_17475/2561472427.py:3: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n  txt = json.loads(response.json())['choices'][0]['message']['content']\n\n\n{'timestamp': '2024-09-28T15:59:06',\n 'location': 'Presidentinlinna',\n 'dimensions': '1280 x 720',\n 'building': 12,\n 'buildings_height_range': '3-7 stories',\n 'car': 30,\n 'truck': 3,\n 'boat': 6,\n 'available_parking_space': 5,\n 'street_lights': 15,\n 'person': 50,\n 'time_of_day': 'afternoon',\n 'artificial_lighting': 'minimal',\n 'visibility_clear': True,\n 'sky_visible': True,\n 'sky_light_conditions': 'daylight',\n 'waterbodies_visible': True,\n 'waterbodies_type': 'harbor'}",
    "crumbs": [
      "Vision",
      "Ask GPT-4v"
    ]
  },
  {
    "objectID": "vision/gpt4v.html#response-format",
    "href": "vision/gpt4v.html#response-format",
    "title": "Ask GPT-4v",
    "section": "Response format",
    "text": "Response format\nInstead of manual transformation, we can try applying GPT structured output to provide a guideline and some suggestions for ouputs. In our application, we have tested with images from city cameras and images from satellite, both of which vastly differ in content with one another. Hence, it might be more relevant if we provide different output formats and field suggestions for GPT separately for each type of image.\nCity landscape output:\nSatellite information output:\n\nsatellite_img = \"/home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_11:39:47_satellite.jpg\"\nImage(satellite_img)\n\n\n\n\n\n\n\n\nLastly, we combine these formats and previous operations into a single function, with an indicator directing which output format to select:\n\nsource\n\nask_gpt4v_about_image_file\n\n ask_gpt4v_about_image_file (path:str, image_type:str='city')\n\nTell all about quantitative information from a given image file\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nPath to the image file\n\n\nimage_type\nstr\ncity\nType of image: “city” or “satellite”, default is “city”\n\n\nReturns\nstr\n\nJSON string with quantitative information\n\n\n\nTest with the previous data files:\n\njson.loads(ask_gpt4v_about_image_file(files[0]))\n\n{'timestamp': '2024-09-28T15:59:06',\n 'location': 'Presidentinlinna',\n 'dimensions': '720x1280',\n 'building': 10,\n 'buildings_height_range': '3-5 stories',\n 'car': 30,\n 'truck': 3,\n 'boat': 6,\n 'available_parking_space': 5,\n 'street_lights': 6,\n 'person': 40,\n 'time_of_day': 'afternoon',\n 'artificial_lighting': 'low',\n 'visibility_clear': True,\n 'sky_visible': True,\n 'sky_light_conditions': 'overcast',\n 'waterbodies_visible': True,\n 'waterbodies_type': 'harbor'}\n\n\n\njson.loads(ask_gpt4v_about_image_file(satellite_img, \"satellite\"))\n\n{'timestamp': '2025-01-16T09:39:24Z',\n 'location': 'Indian Ocean',\n 'latitude': -32.41,\n 'longitude': 47.99,\n 'image_dimensions': '1280x720',\n 'satellite_name': 'International Space Station',\n 'sensor_type': 'Optical',\n 'cloud_cover_percentage': 90,\n 'land_cover_types': ['ocean'],\n 'waterbodies_detected': True,\n 'waterbodies_type': 'Ocean',\n 'urban_areas_detected': False,\n 'lightning_detected': False,\n 'thermal_anomalies_detected': False,\n 'nighttime_imaging': False,\n 'mins_to_sunset': 23,\n 'satellite_speed_kms': '7.6540',\n 'satellite_altitude_km': '427.8'}",
    "crumbs": [
      "Vision",
      "Ask GPT-4v"
    ]
  },
  {
    "objectID": "vision/gpt4v.html#simulated-gpt-workflow",
    "href": "vision/gpt4v.html#simulated-gpt-workflow",
    "title": "Ask GPT-4v",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nThis section tests integrating with our current GPT framework. This function can be used in combination with ytlive module:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\nfrom llmcam.vision.ytlive import *\n\ntools = [\n    function_schema(capture_youtube_live_frame, \"Youtube Live Capture\"),\n    function_schema(select_youtube_live_url, \"Youtube Live URL\"),\n    function_schema(ask_gpt4v_about_image_file, \"GPT4 Vision\"),\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Hi, can you capture a YouTube Live? Use the default link.\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\nCPL CREME tele\ncap_2025.01.16_13:05:52_unclear.jpg\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nHi, can you capture a YouTube Live? Use the default link.\n&gt;&gt; Assistant:\nI have captured an image from the default YouTube Live stream. You can find it stored at the\nfollowing path: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_13:05:52_unclear.jpg`.\n\n\n\n# Continue the conversation and ask about the image file\nmessages.append(form_msg(\"user\", \"Can you extract information about this image?\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nHi, can you capture a YouTube Live? Use the default link.\n&gt;&gt; Assistant:\nI have captured an image from the default YouTube Live stream. You can find it stored at the\nfollowing path: `/home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_13:05:52_unclear.jpg`.\n&gt;&gt; User:\nCan you extract information about this image?\n&gt;&gt; Assistant:\nHere's the information extracted from the captured image:  - **Timestamp**: January 16, 2025, 12:56\nPM - **Location**: Valkosaari - **Image Dimensions**: 1280x720 - **Buildings**: There are 5\nbuildings visible in the image. - **Buildings Height Range**: 2-4 stories - **Water Bodies\nVisible**: Yes - **Type of Water Bodies**: Lake - **Sky Visible**: Yes - **Sky Light Conditions**:\nClear - **Visibility**: Clear - **Time of Day**: Afternoon - **Artificial Lighting**: Low  Feel free\nto ask if you need more details or further assistance!\n\n\nAnother scenario using satellite live:\n\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Capture an image from satellite and extract information about it.\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=xRPjKQtRXR8\n[youtube] xRPjKQtRXR8: Downloading webpage\n[youtube] xRPjKQtRXR8: Downloading ios player API JSON\n[youtube] xRPjKQtRXR8: Downloading tv player API JSON\n[youtube] xRPjKQtRXR8: Downloading m3u8 information\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nCapture an image from satellite and extract information about it.\n&gt;&gt; Assistant:\nThe captured satellite image from the YouTube Live stream provides the following information:  -\n**Timestamp:** 2025-01-16 at 11:06:44 UTC - **Location:** South Atlantic Ocean - **Latitude:**\n-45.07 - **Longitude:** 3.68 - **Image Dimensions:** 1280x720 pixels - **Satellite Name:**\nInternational Space Station - **Sensor Type:** Optical - **Cloud Cover Percentage:** 80% - **Land\nCover Types Detected:** Ocean, Cloud - **Waterbodies Detected:** Yes, specifically Ocean - **Urban\nAreas Detected:** No - **Lightning Detected:** No - **Thermal Anomalies Detected:** No - **Nighttime\nImaging:** No - **Minutes to Sunset:** 28 minutes - **Satellite Speed:** 7.6507 km/s - **Satellite\nAltitude:** 433.2 km  This image mainly shows the ocean with significant cloud cover and no urban\nareas or thermal anomalies.",
    "crumbs": [
      "Vision",
      "Ask GPT-4v"
    ]
  },
  {
    "objectID": "vision/ytlive.html",
    "href": "vision/ytlive.html",
    "title": "YouTube Live Capture",
    "section": "",
    "text": "This experiment focuses on capturing Youtube Live from public cameras monitoring multiple Näkymä Helsingistä locations. We can first extract information about the Youtube Live using the library yt_dlp.\n\nsource\n\n\n\n stream_url (ytlive_url:str, ydl_opts:dict)\n\n\nstream_url(nakyma_helsinkigista_youtube_live_url, ydl_opts)\n\nWARNING: skipping cookie file entry due to invalid length 1: '&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '=======\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '=======\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\\n'\n\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\n\n\n'https://manifest.googlevideo.com/api/manifest/hls_playlist/expire/1737066322/ei/8jKJZ-nWNJyp0u8Ph4j02Qc/ip/82.130.25.234/id/LMZQ7eFhm58.5/itag/232/source/yt_live_broadcast/requiressl/yes/ratebypass/yes/live/1/sgovp/gir%3Dyes%3Bitag%3D136/rqh/1/hdlc/1/hls_chunk_host/rr2---sn-ovgq0oxu-5goe.googlevideo.com/xpc/EgVo2aDSNQ%3D%3D/playlist_duration/3600/manifest_duration/3600/bui/AY2Et-Mi4NLCHEnUJHV_9eqYR8g08BW8IZfGiBVX_kOGk7D2vzSK7U1SO_4Hb_pv0W5mLfmOnwsBer6x/spc/9kzgDYpIzXWTvXc1Vu_1AvYIGD9IHtRMn__-I56lIB7V9sJ72F0P/vprv/1/playlist_type/DVR/initcwndbps/4483750/met/1737044723,/mh/l8/mm/44/mn/sn-ovgq0oxu-5goe/ms/lva/mv/m/mvi/2/pl/18/rms/lva,lva/dover/13/pacing/0/short_key/1/keepalive/yes/fexp/51326932,51335594,51353498,51371294,51384461/mt/1737044203/sparams/expire,ei,ip,id,itag,source,requiressl,ratebypass,live,sgovp,rqh,hdlc,xpc,playlist_duration,manifest_duration,bui,spc,vprv,playlist_type/sig/AJfQdSswRQIhANUtoKCOb5u3JtOLa2QGQR5RQKdJ5PyDc9j8CbfWfBO2AiBaU_ySIFkC7dvK2VlOBWeKwYWtBITdimZW6sScNBTttw%3D%3D/lsparams/hls_chunk_host,initcwndbps,met,mh,mm,mn,ms,mv,mvi,pl,rms/lsig/AGluJ3MwRgIhAOk99P4NUNKgNCKsLE11eZe62AeQ_6DPuvxkKxi3sr7cAiEAmVHKhNZ1OZV3HpKU5L71VjOY5SkV05GYwJblbKW-nEk%3D/playlist/index.m3u8'\n\n\nAt this point, we have obtained a special link for downloading captures from this livestream. This URL is a dynamically generated link used by YouTube for delivering live streaming content via an HLS (HTTP Live Streaming) protocol. We can proceed to donwload content chunks from this link with cv2.\nNOTE: This livestream contains a small frame at top left corner to display the current time and location. We can attempt to crop this small frame and extract further information from the captured image.\n\nsource\n\n\n\n\n show_frame (frame)\n\n\nsource\n\n\n\n\n crop_frame (frame, crop=(0, 0, 480, 30))\n\n\nsource\n\n\n\n\n frame_to_text (frame)\n\n\nsource\n\n\n\n\n known (txt:str, known_places:str)\n\nTry to find one of known_places are included in the given txt\n\nsource\n\n\n\n\n meta (frame, known_places=['Olympiaterminaali', 'Etelasatama',\n       'Eteladsatama', 'Presidentinlinna', 'Tuomiokirkko', 'Kauppatori',\n       'Kauppator', 'Torni', 'Valkosaari'], printing=False)\n\nWithdraw meta data, datetime & place\n\nsource\n\n\n\n\n fname (prefix, dt, pl)\n\nTest workflow combining the above utilities and try saving the downloaded file in a local directory:\n\nurl = stream_url(nakyma_helsinkigista_youtube_live_url, ydl_opts)\ncap = cv2.VideoCapture(url)\nret, frame = cap.read()\nif ret:\n    print(\"Cropped frame:\")\n    show_frame(crop_frame(frame))\n\n    print(\"Text in cropped frame:\", frame_to_text(crop_frame(frame)))\n    try:\n        file_name = fname(\"cap_\", *meta(crop_frame(frame), printing=True))\n    except:\n        file_name = fname(\"fail_\", datetime.now(), \"nowhere\")\n    print(\"File name:\", file_name)\n    path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\"))/file_name\n    path.parent.mkdir(parents=True, exist_ok=True)\n    cv2.imwrite(path, frame)\n    print(\"Saved to:\", path)\nelse:\n    print(\"Failed to capture frame.\")\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\nCropped frame:\n\n\n\n\n\n\n\n\n\nText in cropped frame: 16.01.2025 18:16:24 Tuomiokirkko\n16.01.2025 18:16:24 Tuomiokirkko\nFile name: cap_2025.01.16_18:16:24_Tuomiokirkko.jpg\nSaved to: /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:16:24_Tuomiokirkko.jpg\n\n\nAs such, we can follow this guideline for implementing the modularized functions:\n\nExtract HSL URL from Youtube Live\n\nCapture live images with this URL\n\nCrop small frame and extract further metadata from captured images\n\nForm suitable file names and save captured images",
    "crumbs": [
      "Vision",
      "YouTube Live Capture"
    ]
  },
  {
    "objectID": "vision/ytlive.html#youtube-live-capture-experiment",
    "href": "vision/ytlive.html#youtube-live-capture-experiment",
    "title": "YouTube Live Capture",
    "section": "",
    "text": "This experiment focuses on capturing Youtube Live from public cameras monitoring multiple Näkymä Helsingistä locations. We can first extract information about the Youtube Live using the library yt_dlp.\n\nsource\n\n\n\n stream_url (ytlive_url:str, ydl_opts:dict)\n\n\nstream_url(nakyma_helsinkigista_youtube_live_url, ydl_opts)\n\nWARNING: skipping cookie file entry due to invalid length 1: '&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '=======\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '=======\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\\n'\nWARNING: skipping cookie file entry due to invalid length 1: '&gt;&gt;&gt;&gt;&gt;&gt;&gt; Stashed changes\\n'\n\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\n\n\n'https://manifest.googlevideo.com/api/manifest/hls_playlist/expire/1737066322/ei/8jKJZ-nWNJyp0u8Ph4j02Qc/ip/82.130.25.234/id/LMZQ7eFhm58.5/itag/232/source/yt_live_broadcast/requiressl/yes/ratebypass/yes/live/1/sgovp/gir%3Dyes%3Bitag%3D136/rqh/1/hdlc/1/hls_chunk_host/rr2---sn-ovgq0oxu-5goe.googlevideo.com/xpc/EgVo2aDSNQ%3D%3D/playlist_duration/3600/manifest_duration/3600/bui/AY2Et-Mi4NLCHEnUJHV_9eqYR8g08BW8IZfGiBVX_kOGk7D2vzSK7U1SO_4Hb_pv0W5mLfmOnwsBer6x/spc/9kzgDYpIzXWTvXc1Vu_1AvYIGD9IHtRMn__-I56lIB7V9sJ72F0P/vprv/1/playlist_type/DVR/initcwndbps/4483750/met/1737044723,/mh/l8/mm/44/mn/sn-ovgq0oxu-5goe/ms/lva/mv/m/mvi/2/pl/18/rms/lva,lva/dover/13/pacing/0/short_key/1/keepalive/yes/fexp/51326932,51335594,51353498,51371294,51384461/mt/1737044203/sparams/expire,ei,ip,id,itag,source,requiressl,ratebypass,live,sgovp,rqh,hdlc,xpc,playlist_duration,manifest_duration,bui,spc,vprv,playlist_type/sig/AJfQdSswRQIhANUtoKCOb5u3JtOLa2QGQR5RQKdJ5PyDc9j8CbfWfBO2AiBaU_ySIFkC7dvK2VlOBWeKwYWtBITdimZW6sScNBTttw%3D%3D/lsparams/hls_chunk_host,initcwndbps,met,mh,mm,mn,ms,mv,mvi,pl,rms/lsig/AGluJ3MwRgIhAOk99P4NUNKgNCKsLE11eZe62AeQ_6DPuvxkKxi3sr7cAiEAmVHKhNZ1OZV3HpKU5L71VjOY5SkV05GYwJblbKW-nEk%3D/playlist/index.m3u8'\n\n\nAt this point, we have obtained a special link for downloading captures from this livestream. This URL is a dynamically generated link used by YouTube for delivering live streaming content via an HLS (HTTP Live Streaming) protocol. We can proceed to donwload content chunks from this link with cv2.\nNOTE: This livestream contains a small frame at top left corner to display the current time and location. We can attempt to crop this small frame and extract further information from the captured image.\n\nsource\n\n\n\n\n show_frame (frame)\n\n\nsource\n\n\n\n\n crop_frame (frame, crop=(0, 0, 480, 30))\n\n\nsource\n\n\n\n\n frame_to_text (frame)\n\n\nsource\n\n\n\n\n known (txt:str, known_places:str)\n\nTry to find one of known_places are included in the given txt\n\nsource\n\n\n\n\n meta (frame, known_places=['Olympiaterminaali', 'Etelasatama',\n       'Eteladsatama', 'Presidentinlinna', 'Tuomiokirkko', 'Kauppatori',\n       'Kauppator', 'Torni', 'Valkosaari'], printing=False)\n\nWithdraw meta data, datetime & place\n\nsource\n\n\n\n\n fname (prefix, dt, pl)\n\nTest workflow combining the above utilities and try saving the downloaded file in a local directory:\n\nurl = stream_url(nakyma_helsinkigista_youtube_live_url, ydl_opts)\ncap = cv2.VideoCapture(url)\nret, frame = cap.read()\nif ret:\n    print(\"Cropped frame:\")\n    show_frame(crop_frame(frame))\n\n    print(\"Text in cropped frame:\", frame_to_text(crop_frame(frame)))\n    try:\n        file_name = fname(\"cap_\", *meta(crop_frame(frame), printing=True))\n    except:\n        file_name = fname(\"fail_\", datetime.now(), \"nowhere\")\n    print(\"File name:\", file_name)\n    path = Path(os.getenv(\"LLMCAM_DATA\", \"../data\"))/file_name\n    path.parent.mkdir(parents=True, exist_ok=True)\n    cv2.imwrite(path, frame)\n    print(\"Saved to:\", path)\nelse:\n    print(\"Failed to capture frame.\")\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\nCropped frame:\n\n\n\n\n\n\n\n\n\nText in cropped frame: 16.01.2025 18:16:24 Tuomiokirkko\n16.01.2025 18:16:24 Tuomiokirkko\nFile name: cap_2025.01.16_18:16:24_Tuomiokirkko.jpg\nSaved to: /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:16:24_Tuomiokirkko.jpg\n\n\nAs such, we can follow this guideline for implementing the modularized functions:\n\nExtract HSL URL from Youtube Live\n\nCapture live images with this URL\n\nCrop small frame and extract further metadata from captured images\n\nForm suitable file names and save captured images",
    "crumbs": [
      "Vision",
      "YouTube Live Capture"
    ]
  },
  {
    "objectID": "vision/ytlive.html#modularize-with-live-instances",
    "href": "vision/ytlive.html#modularize-with-live-instances",
    "title": "YouTube Live Capture",
    "section": "Modularize with Live instances",
    "text": "Modularize with Live instances\nThis section implements the modularized functions with Object-Oriented Programming approach (OOP). Each Youtube Live is managed by a YTLive instance, which captures images from its live stream when prompted.\n\nGeneral Live instance\nNOTE: For a generic Youtube Live stream, step 3 in the described workflow is skipped and file name is generated as a combination of the current timestamp and user-input location.\n\nsource\n\n\nYTLive\n\n YTLive (url:str, data_dir:Optional[pathlib.Path]=Path('../data'),\n         place:Optional[str]='nowhere')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nYouTube Live URL\n\n\ndata_dir\nOptional\n../data\nDirectory to store captured images\n\n\nplace\nOptional\nnowhere\nLocation name\n\n\n\nTest usage with a different live stream of Santa Claus Village:\n\nSantaClausVillage = YTLive(url=\"https://www.youtube.com/live/Cp4RRAEgpeU?si=IwqJ4QU1Umv9PdgW\", place=\"santaclausvillage\")\nfile = SantaClausVillage()\nprint()\nprint(\"File path:\", file)\n\n[youtube] Extracting URL: https://www.youtube.com/live/Cp4RRAEgpeU?si=IwqJ4QU1Umv9PdgW\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading tv player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n\nFile path: /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:25:30_santaclausvillage.jpg\n\n\n\n\nNäkymä Helsingistä Live instance\nWe also add a special support for Näkymä Helsingistä livestream that utilizes the small frame at top left corner to extract metadata and generate file names. This is a subclass that inherits all other functions from YTLive while modifying the file_name function.\n\nsource\n\n\nNHsta\n\n NHsta (url:Optional[str]='https://www.youtube.com/watch?v=LMZQ7eFhm58',\n        data_dir:Optional[pathlib.Path]=Path('../data'),\n        place:Optional[str]='unclear')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nOptional\nhttps://www.youtube.com/watch?v=LMZQ7eFhm58\nYouTube Live URL\n\n\ndata_dir\nOptional\n../data\nDirectory to store captured images\n\n\nplace\nOptional\nunclear\nLocation name if OCR fails\n\n\n\nTest usage:\n\nNakymaHelsingista = NHsta()\nfile = NakymaHelsingista()\nprint()\nprint(\"File path:\", file)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\n16.01.2025 18:16:29 Tuomiokirkko\n\nFile path: /home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:16:29_Tuomiokirkko.jpg\n\n\nTry showing the cropped small frame from saved file:\n\ndef crop_image(path, crop=(0, 0, 480, 30)): return Image.open(path).crop(crop)\ndef show_image(path):\n    plt.imshow(crop_image(path))\n    plt.axis('off')\n    plt.show()\n\nshow_image(file)\n\n\n\n\n\n\n\n\n\n\nGeneral-purpose function for Function calling\nFor simplification purpose, we implement a general-purpose function that initiate these Live instances and download file to be used in GPT Function calling:\n\nsource\n\n\ncapture_youtube_live_frame\n\n capture_youtube_live_frame (link:Optional[str]=None,\n                             place:Optional[str]=None)\n\nCapture a jpeg image from YouTube Live and return the path to the saved image\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlink\nOptional\nNone\nYouTube Live link\n\n\nplace\nOptional\nNone\nLocation of live image\n\n\nReturns\nstr\n\nPath to the saved image\n\n\n\n\n\nUtility function for default Youtube Links\nUtility function for saving and retrieving some notable Youtube links.\n\nsource\n\n\nselect_youtube_live_url\n\n select_youtube_live_url (location:Optional[str]='Helsinki')\n\nSelect the Youtube Live URL based on the location name\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlocation\nOptional\nHelsinki\nLocation name, one of [“santaclausvillage”, “parkinglot”, “helsinki”, “satellite”]",
    "crumbs": [
      "Vision",
      "YouTube Live Capture"
    ]
  },
  {
    "objectID": "vision/ytlive.html#simulated-gpt-workflow",
    "href": "vision/ytlive.html#simulated-gpt-workflow",
    "title": "YouTube Live Capture",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ntools = [function_schema(capture_youtube_live_frame, \"Youtube Live Capture\")]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Hi, can you capture YouTube Live?\")\n])\ncomplete(messages, tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nHi, can you capture YouTube Live?\n&gt;&gt; Assistant:\nYes, I can capture an image from a YouTube Live stream. Please provide the YouTube Live link, and if\napplicable, specify the location or place you want the image to represent.\n\n\n\n# Continue the conversation\nmessages.append(form_msg(\"user\", \"You can use the default link.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading tv player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\n16.01.2025 18:16:39 Tuomiokirkko\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nHi, can you capture YouTube Live?\n&gt;&gt; Assistant:\nYes, I can capture an image from a YouTube Live stream. Please provide the YouTube Live link, and if\napplicable, specify the location or place you want the image to represent.\n&gt;&gt; User:\nYou can use the default link.\n&gt;&gt; Assistant:\nI have captured an image from the default YouTube Live stream. You can view it\n[here](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:16:39_Tuomiokirkko.jpg).\n\n\nAnother scenario (selecting with location):\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ntools = [\n    function_schema(capture_youtube_live_frame, \"Youtube Live Capture\"),\n    function_schema(select_youtube_live_url, \"Select Youtube Live URL\")\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Hi, can you capture an image from Santa Claus Village?\")\n])\ncomplete(messages, tools)\nprint_msgs(messages)\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=Cp4RRAEgpeU\n[youtube] Cp4RRAEgpeU: Downloading webpage\n[youtube] Cp4RRAEgpeU: Downloading ios player API JSON\n[youtube] Cp4RRAEgpeU: Downloading tv player API JSON\n[youtube] Cp4RRAEgpeU: Downloading m3u8 information\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nHi, can you capture an image from Santa Claus Village?\n&gt;&gt; Assistant:\nI've captured an image from Santa Claus Village for you. You can view it here: [Santa Claus Village\nLive\nImage](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2025.01.16_18:25:47_santaclausvillage.jpg).",
    "crumbs": [
      "Vision",
      "YouTube Live Capture"
    ]
  },
  {
    "objectID": "vision/dtcam.html",
    "href": "vision/dtcam.html",
    "title": "DTCam",
    "section": "",
    "text": "This utility focuses on capturing images from a weathercam provided by Road DigiTraffic based on its station location.\nRoad DigiTraffic provides data about public cameras from their weathercam stations, which includes information about its region name. Therefore, we can filter stations by their region name and find specific stations in a certain region (e.g., Helsinki, Porvoo, …).",
    "crumbs": [
      "Vision",
      "DTCam"
    ]
  },
  {
    "objectID": "vision/dtcam.html#camera-address-book",
    "href": "vision/dtcam.html#camera-address-book",
    "title": "DTCam",
    "section": "Camera address book",
    "text": "Camera address book\nFirst, we can build an address book of camera installed address, and image capture url from Road DigiTraffic. The format of this address book can be a list of JSON-formatted data objects:\n[\n    {\n        \"address\": &lt;camera installed location&gt;,\n        \"url\": &lt;capture image url&gt;,\n    },\n    {\n        \"address\": &lt;camera installed location&gt;,\n        \"url\": &lt;capture image url&gt;,\n    },\n    ....\n]\n\nsource\n\nbuild_address_book\n\n build_address_book (stations_url:str)\n\n\nsource\n\n\naddress_book\n\n address_book (stations_url:str='https://tie.digitraffic.fi/api/weathercam\n               /v1/stations', force_update=False)\n\nGet an address book [{“address”:, “url”:}]\n\nsource\n\n\ncamera_address_book\n\n camera_address_book (stations_url:Optional[str]='https://tie.digitraffic.\n                      fi/api/weathercam/v1/stations',\n                      update:Optional[bool]=False)\n\nGet weather camera location addressbook of camera location:image url. You can get an capture of camera from this url\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstations_url\nOptional\nhttps://tie.digitraffic.fi/api/weathercam/v1/stations\nWeather Camera URL\n\n\nupdate\nOptional\nFalse\nUpdate addressbook forcely\n\n\nReturns\nstr\n\nList of \"camera location\":\"image url\"\n\n\n\nExample address book from Road DigiTraffic:\n\naddresses = camera_address_book()\njson.loads(addresses)[0]\n\n{'address': 'Porvoo,Tie 7 Porvoo, Harabacka,Porvoo,Uusimaa',\n 'url': 'https://weathercam.digitraffic.fi/C0150200.jpg'}",
    "crumbs": [
      "Vision",
      "DTCam"
    ]
  },
  {
    "objectID": "vision/dtcam.html#weathercam-image-capturing",
    "href": "vision/dtcam.html#weathercam-image-capturing",
    "title": "DTCam",
    "section": "Weathercam image capturing",
    "text": "Weathercam image capturing\nBased on possible weathercam stations, we extract images from these cameras by requesting the corresponding URL:\n\nsource\n\nstations\n\n stations (key:str)\n\n“Get all weather station including key word\nExample with finding stations in Porvoo:\n\nPorvoos = stations(\"Porvoo\")\nPorvoos[0]\n\n{'type': 'Feature',\n 'id': 'C01502',\n 'geometry': {'type': 'Point', 'coordinates': [25.616391, 60.390238, 0.0]},\n 'properties': {'id': 'C01502',\n  'name': 'vt7_Porvoo_Harabacka',\n  'collectionStatus': 'GATHERING',\n  'state': None,\n  'dataUpdatedTime': '2024-12-12T03:26:57Z',\n  'presets': [{'id': 'C0150200', 'inCollection': True}]}}\n\n\nFor DigiTraffic, the image from a weather camera can be retrieved with a weathercam URL using the presets data:\n\nsource\n\n\npresets\n\n presets (station:dict)\n\nGet all presets at a given weather station\n\npreset = presets(Porvoos[0])[0]\nimageUrl = preset['imageUrl']\nimageUrl\n\n'https://weathercam.digitraffic.fi/C0150200.jpg'\n\n\nDownload and save this image to local disk:\n\nsource\n\n\ncapture\n\n capture (preset:dict)\n\nCapture image at a given preset location in a Weather station, and return an image path\n\nhdr, path = capture(preset)\nhdr\n\n{'Content-Type': 'image/jpeg', 'Content-Length': '66682', 'Connection': 'keep-alive', 'x-amz-id-2': 'e1nXLX4h6A1+r45u0fdlv7R44L3NqThwbK0/xJ7cUHnPUzkR6z4GhW5s0HhjppQttrBA0mLj+DoGPEfuI5ARaQ==', 'x-amz-request-id': '7YJ7D76VBEJA18S2', 'last-modified': 'Thu, 12 Dec 2024 13:03:04 GMT', 'x-amz-expiration': 'expiry-date=\"Sat, 14 Dec 2024 00:00:00 GMT\", rule-id=\"Delete versions and current images after 24h\"', 'x-amz-server-side-encryption': 'AES256', 'X-Amz-Meta-Last-Modified': 'Thu, 12 Dec 2024 13:03:04 GMT', 'x-amz-version-id': 'MbujShinwFlbCASbDJ4XE957WpwwBYT4', 'Accept-Ranges': 'bytes', 'Server': 'AmazonS3', 'Date': 'Thu, 12 Dec 2024 13:10:19 GMT', 'ETag': '\"1c38821750ddec3363436dcd1077eb83\"', 'Vary': 'Accept-Encoding', 'X-Cache': 'Hit from cloudfront', 'Via': '1.1 95ad0c949c0fe7e97075c6690b8574aa.cloudfront.net (CloudFront)', 'X-Amz-Cf-Pop': 'HEL51-P1', 'Alt-Svc': 'h3=\":443\"; ma=86400', 'X-Amz-Cf-Id': 'p4oSYL7h9m8R9Xua8kSJe59PlJJuqITUAx_V5e9urJ3GJSoAzOEu_w==', 'Age': '54'}\n\n\n\ndisplay(Image.open(path))\n\n\n\n\n\n\n\n\nWe can all previous utilities and implement a function that capture images based on location keyword:\n\nsource\n\n\ncap\n\n cap (key:str='Porvoo')\n\nCapture an image at specified location, save it, and return its path\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nkey\nstr\nPorvoo\nLocation keyword\n\n\nReturns\nstr\n\nImage path\n\n\n\nTest capturing an image of Porvoo:\n\npath = cap(\"Porvoo\")\ndisplay(Image.open(path))",
    "crumbs": [
      "Vision",
      "DTCam"
    ]
  },
  {
    "objectID": "vision/dtcam.html#simulated-gpt-workflow",
    "href": "vision/dtcam.html#simulated-gpt-workflow",
    "title": "DTCam",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nThis section tests integrating with our current GPT framework. This function can be used in combination with previous functions in the vision module:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\nfrom llmcam.vision.ytlive import capture_youtube_live_frame\nfrom llmcam.vision.gpt4v import ask_gpt4v_about_image_file\nfrom llmcam.vision.yolo import detect_objects\n\ntools = [\n    function_schema(capture_youtube_live_frame, \"Youtube Live Capture\"),\n    function_schema(ask_gpt4v_about_image_file, \"GPT4 Vision\"),\n    function_schema(detect_objects, \"YOLO Object Detection\"),\n    function_schema(camera_address_book, \"Weather Camera Address Book\"),\n    function_schema(cap, \"Weather Station Capture\")\n]\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Name some locations to capture weather station images from the default address book.\"),\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nName some locations to capture weather station images from the default address book.\n&gt;&gt; Assistant:\nHere are some locations from the default weather camera address book where you can capture images:\n1. Porvoo, Uusimaa - [Link](https://weathercam.digitraffic.fi/C0150200.jpg) 2. Inkoo, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150301.jpg) 3. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150302.jpg) 4. Karkkila, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150401.jpg) 5. Helsinki, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150402.jpg) 6. Lohja, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150601.jpg) 7. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150701.jpg) 8. Mäntsälä, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150801.jpg) 9. Hyvinkää, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150901.jpg) 10. Vantaa, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0153201.jpg)  These links provide current weather images\nfrom various locations across Finland.\n\n\n\n# Continue the conversation and ask the user about the image\nmessages.append(form_msg(\"user\", \"Capture a weather station image from Helsinki and tell me some basic information about it.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nName some locations to capture weather station images from the default address book.\n&gt;&gt; Assistant:\nHere are some locations from the default weather camera address book where you can capture images:\n1. Porvoo, Uusimaa - [Link](https://weathercam.digitraffic.fi/C0150200.jpg) 2. Inkoo, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150301.jpg) 3. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150302.jpg) 4. Karkkila, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150401.jpg) 5. Helsinki, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150402.jpg) 6. Lohja, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150601.jpg) 7. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150701.jpg) 8. Mäntsälä, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150801.jpg) 9. Hyvinkää, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150901.jpg) 10. Vantaa, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0153201.jpg)  These links provide current weather images\nfrom various locations across Finland.\n&gt;&gt; User:\nCapture a weather station image from Helsinki and tell me some basic information about it.\n&gt;&gt; Assistant:\nI captured an image from a weather station in Helsinki at the location \"Tie 101 Helsinki, Pakila,\"\nspecifically at Kehä 1 Itään. Here are some basic details about the image:  - **Dimensions:** 1280 x\n720 - **Time of Day:** Afternoon - **Visibility:** Clear - **Sky Light Conditions:** Daylight -\n**Number of Cars:** 10 - **Street Lights:** 5 - **Buildings:** No buildings visible - **Artificial\nLighting:** Minimal - **Waterbodies Visible:** No  The image shows a well-lit area with clear\nvisibility, and there are no people, trucks, boats, or available parking spaces visible.\n\n\n\n# Continue the conversation and detect objects in the image\nmessages.append(form_msg(\"user\", \"Can you detect what objects are captured in this image?\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.12_13:06:53_Kehä 1 Itään_C0151301.jpg: 384x640 7 cars, 54.9ms\nSpeed: 5.5ms preprocess, 54.9ms inference, 221.6ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nName some locations to capture weather station images from the default address book.\n&gt;&gt; Assistant:\nHere are some locations from the default weather camera address book where you can capture images:\n1. Porvoo, Uusimaa - [Link](https://weathercam.digitraffic.fi/C0150200.jpg) 2. Inkoo, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150301.jpg) 3. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150302.jpg) 4. Karkkila, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150401.jpg) 5. Helsinki, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150402.jpg) 6. Lohja, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150601.jpg) 7. Hanko, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150701.jpg) 8. Mäntsälä, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150801.jpg) 9. Hyvinkää, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0150901.jpg) 10. Vantaa, Uusimaa -\n[Link](https://weathercam.digitraffic.fi/C0153201.jpg)  These links provide current weather images\nfrom various locations across Finland.\n&gt;&gt; User:\nCapture a weather station image from Helsinki and tell me some basic information about it.\n&gt;&gt; Assistant:\nI captured an image from a weather station in Helsinki at the location \"Tie 101 Helsinki, Pakila,\"\nspecifically at Kehä 1 Itään. Here are some basic details about the image:  - **Dimensions:** 1280 x\n720 - **Time of Day:** Afternoon - **Visibility:** Clear - **Sky Light Conditions:** Daylight -\n**Number of Cars:** 10 - **Street Lights:** 5 - **Buildings:** No buildings visible - **Artificial\nLighting:** Minimal - **Waterbodies Visible:** No  The image shows a well-lit area with clear\nvisibility, and there are no people, trucks, boats, or available parking spaces visible.\n&gt;&gt; User:\nCan you detect what objects are captured in this image?\n&gt;&gt; Assistant:\nThe image captured at Helsinki's weather station contains 7 cars. No other objects were detected in\nthe image.",
    "crumbs": [
      "Vision",
      "DTCam"
    ]
  },
  {
    "objectID": "demos/demo_scenario_3.html",
    "href": "demos/demo_scenario_3.html",
    "title": "Demo Scenario 3",
    "section": "",
    "text": "The goal of this scenario is to demonstrate built-in GPT Function calling tools for building a monitor pipeline in a relevant use case.\nThe scenario of this demo is a user wanting to build a pipeline to monitor visual data from a Youtube Live (or any streaming services). Without our application, the user would need to manually integrate and configure various tools, write custom code to connect them to the streaming service, and ensure the system is able to process and analyze the live visual feed in real-time. This approach can be complex, time-consuming, and prone to errors, especially without deep technical knowledge. Additionally, maintaining a smooth and efficient pipeline that handles real-time visual data poses a significant challenge.\nOur application simplifies this process by automating the integration and configuration of the necessary tools. In the preparation stage, the user only needs to set up primitive tools or import from our default functions. From then, the user can easily design and manage the pipeline using plain language instructions, while the GPT FC handles the integration and seamlessly chains complex processes. This enables the user to monitor live visual data in real time without the need for extensive coding or technical expertise, making the process more efficient and accessible.\nIn this demo, we will monitor the number of cars and available lots in the parking lots shown in the Youtube Live: https://www.youtube.com/watch?v=mwN6l3O1MNI.",
    "crumbs": [
      "Demos",
      "Demo Scenario 3"
    ]
  },
  {
    "objectID": "demos/demo_scenario_3.html#step-1---preparation",
    "href": "demos/demo_scenario_3.html#step-1---preparation",
    "title": "Demo Scenario 3",
    "section": "Step 1 - Preparation",
    "text": "Step 1 - Preparation\nFor this demo, we will use the built-in tools from llmcam.vision modules and integrate the simple notification system from llmcam.utils.notification.\nAt the preparation step, the main tasks include:\n\nImport necessary built-in tools from llmcam.core, llmcam.vision, and llmcam.utils.notification modules.\n\nSet up initial tools list and add built-in tools from llmcam.vision.\n\nSet up notification systems as instructed in utils/03_notification.ipynb.\n\nVerify set up with GPT messages.\n\n\n# Import all the necessary modules\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import *\nfrom llmcam.utils.notification import *\nfrom llmcam.vision.ytlive import *\nfrom llmcam.vision.gpt4v import *\nfrom llmcam.vision.yolo import *\n\n\n# Define the `tools` list with default built-in functions\ntools = [function_schema(func, \"llmcam_vision\") for func in [\n    capture_youtube_live_frame, \n    ask_gpt4v_about_image_file,\n    detect_objects\n]]\n\n\n# Define the `send_notification` function that adds a message to the `notifications` list\nnotifications = []\ndef send_notification(msg: str):\n    \"\"\"Send a notification\"\"\"\n    notifications.append(msg)\n    return notifications\n\n# Define a function that starts the notification stream using the default functions\ndef start_notification_stream(\n    messages: list  # Previous conversation with the user\n):\n    return notification_stream_core(\n        tools, \n        messages,\n        stream_starter=default_stream_starter,  # Default stream starter from `llmcam.utils.notification` module\n        send_notification=send_notification,\n        stream_stopper=default_stream_stopper  # Default stream stopper from `llmcam.utils.notification` module\n    )\n\n# Add the start_notification_stream function to the tools list\ntools.append(process_notification_schema(start_notification_stream))\n\n\n# Start the conversation and verify the tools\nmessages = form_msgs([\n    # For consistency performance, describe in details how the system should work\n    ('system', 'You are a helpful system administrator. Use the supplied tools to assist the user. \\\nIf asked to monitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream instead.'),\n    ('user', 'What tools can you use?'),\n])\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI can use the following tools:  1. **capture_youtube_live_frame**: Capture a JPEG image from a\nYouTube Live stream.  2. **ask_gpt4v_about_image_file**: Provide detailed quantitative information\nabout a given image file.  3. **detect_objects**: Detect objects in an image using the YOLO model.\n4. **start_notification_stream**: Begin monitoring and notifying you about certain events or\nconditions based on our conversation.  5. **multi_tool_use.parallel**: Execute multiple tools in\nparallel, allowing for simultaneous operations when possible.",
    "crumbs": [
      "Demos",
      "Demo Scenario 3"
    ]
  },
  {
    "objectID": "demos/demo_scenario_3.html#step-2---test-ml-pipeline-with-a-single-real-time-image",
    "href": "demos/demo_scenario_3.html#step-2---test-ml-pipeline-with-a-single-real-time-image",
    "title": "Demo Scenario 3",
    "section": "Step 2 - Test ML pipeline with a single real-time image",
    "text": "Step 2 - Test ML pipeline with a single real-time image\nWe first test the pipeline with a single usage (not streamlined into the monitoring stream). This testing phase will proceed as follows:\n\nCapture an image from the parking lot.\n\nExtract basic information and (if possible) the number of total parking lots.\n\nDetect the number of cars (with YOLO) and (if possible) the number of current available parking lots.\n\n\n# Capture a frame from the YouTube live stream\nmessages.append(form_msg(\n    'user', \n    'Capture an image of the parking lots in https://www.youtube.com/watch?v=mwN6l3O1MNI. \\\nTell me also the file path of the captured image.'))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=mwN6l3O1MNI\n[youtube] mwN6l3O1MNI: Downloading webpage\n[youtube] mwN6l3O1MNI: Downloading ios player API JSON\n[youtube] mwN6l3O1MNI: Downloading mweb player API JSON\n[youtube] mwN6l3O1MNI: Downloading m3u8 information\n[youtube] mwN6l3O1MNI: Downloading m3u8 information\n&gt;&gt; Assistant:\nThe image of the parking lots from the YouTube Live stream has been captured. The file path of the\ncaptured image is `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_04:48:27_parking lots.jpg`.\n\n\n\n# Display the captured image\nfrom IPython.display import Image\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_04:48:27_parking lots.jpg\")\n\n\n\n\n\n\n\n\n\n# Ask GPT-4 Vision about the captured image\nmessages.append(form_msg(\"user\", \"Tell me about the number of total parking lots and any extra information.\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nThe captured image of the parking lots contains the following information:  - **Dimensions of the\nimage**: 1280 x 720 pixels - **Total Buildings**: 8, ranging from 2 to 3 stories in height -\n**Vehicles**: 14 cars and 1 truck - **Available Parking Spaces**: 6 - **Street Lights**: 12 -\n**People**: None visible in the image - **Time of Day**: Night - **Lighting**: Prominent artificial\nlighting - **Visibility**: Clear - **Sky Visibility**: The sky is not visible in the image.\n\n\n\n# Detect objects in the captured image\nmessages.append(form_msg(\n    \"user\", \n    \"Detect the number of cars and the currently available parking lots in the image.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_04:48:27_parking lots.jpg: 384x640 18 cars, 1 umbrella, 48.4ms\nSpeed: 3.2ms preprocess, 48.4ms inference, 210.6ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; Assistant:\nThe image analysis using object detection reveals:  - **Number of Cars**: 18 - **Umbrella**: 1\nPreviously, it was noted there were 6 available parking spaces. However, with the discrepancy in the\nnumber of detected cars (18 vs. previously noted 14), it's possible that the detection might account\nfor vehicles outside the originally anticipated area or visibility factors vary in automated\ndetection. Therefore, an accurate count of available parking spaces might need manual verification\nor confirmation against the expected lot capacity and layout.\n\n\n\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/detection_cap_2024.12.20_04:48:27_parking lots.jpg\")",
    "crumbs": [
      "Demos",
      "Demo Scenario 3"
    ]
  },
  {
    "objectID": "demos/demo_scenario_3.html#step-3---streamline-the-detection-process-with-monitoring-stream-and-notifications",
    "href": "demos/demo_scenario_3.html#step-3---streamline-the-detection-process-with-monitoring-stream-and-notifications",
    "title": "Demo Scenario 3",
    "section": "Step 3 - Streamline the detection process with monitoring stream and notifications",
    "text": "Step 3 - Streamline the detection process with monitoring stream and notifications\nAfter validating that the pipeline works, we can streamline this whole process with a continuous monitoring stream and sending notifications on certain events using the built-in notification stream framework. At this step, the user can simply ask the GPT to start a separate monitoring stream for notifying certain events. The user can also set a stopping criterion, e.g., stop after x notifications, stop after 3 days, …\n\n# Start the notification stream\nmessages.append(form_msg(\n    \"user\", \n    \"Start a separate notification stream and notify me whenever there are more than 10 cars in the parking lots. \\\nStop after 3 notifications.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI've started a separate notification stream to notify you whenever there are more than 10 cars in\nthe parking lots. This will stop after 3 notifications.\n\n\nAs noticeable here, the monitoring process is detached from the main thread as logs from YOLO detection are not displayed.\n\n# Check the notifications\nfor msg in notifications:\n    print(msg)\n\nNotification stream initiated for car count monitoring in the parking lots.\nWarning: There are more than 10 cars in the parking lots.\nWarning: There are more than 10 cars in the parking lots.\nWarning: There are more than 10 cars in the parking lots.",
    "crumbs": [
      "Demos",
      "Demo Scenario 3"
    ]
  },
  {
    "objectID": "demos/demo_scenario_3.html#conversation-transcript",
    "href": "demos/demo_scenario_3.html#conversation-transcript",
    "title": "Demo Scenario 3",
    "section": "CONVERSATION TRANSCRIPT",
    "text": "CONVERSATION TRANSCRIPT\n\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user. If asked to\nmonitor and notify, do not resolve in main thread but defer to sub GPT in notifcation stream\ninstead.\n&gt;&gt; User:\nWhat tools can you use?\n&gt;&gt; Assistant:\nI can use the following tools:  1. **capture_youtube_live_frame**: Capture a JPEG image from a\nYouTube Live stream.  2. **ask_gpt4v_about_image_file**: Provide detailed quantitative information\nabout a given image file.  3. **detect_objects**: Detect objects in an image using the YOLO model.\n4. **start_notification_stream**: Begin monitoring and notifying you about certain events or\nconditions based on our conversation.  5. **multi_tool_use.parallel**: Execute multiple tools in\nparallel, allowing for simultaneous operations when possible.\n&gt;&gt; User:\nCapture an image of the parking lots in https://www.youtube.com/watch?v=mwN6l3O1MNI. Tell me also\nthe file path of the captured image.\n&gt;&gt; Assistant:\nThe image of the parking lots from the YouTube Live stream has been captured. The file path of the\ncaptured image is `/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.20_04:48:27_parking lots.jpg`.\n&gt;&gt; User:\nTell me about the number of total parking lots and any extra information.\n&gt;&gt; Assistant:\nThe captured image of the parking lots contains the following information:  - **Dimensions of the\nimage**: 1280 x 720 pixels - **Total Buildings**: 8, ranging from 2 to 3 stories in height -\n**Vehicles**: 14 cars and 1 truck - **Available Parking Spaces**: 6 - **Street Lights**: 12 -\n**People**: None visible in the image - **Time of Day**: Night - **Lighting**: Prominent artificial\nlighting - **Visibility**: Clear - **Sky Visibility**: The sky is not visible in the image.\n&gt;&gt; User:\nDetect the number of cars and the currently available parking lots in the image.\n&gt;&gt; Assistant:\nThe image analysis using object detection reveals:  - **Number of Cars**: 18 - **Umbrella**: 1\nPreviously, it was noted there were 6 available parking spaces. However, with the discrepancy in the\nnumber of detected cars (18 vs. previously noted 14), it's possible that the detection might account\nfor vehicles outside the originally anticipated area or visibility factors vary in automated\ndetection. Therefore, an accurate count of available parking spaces might need manual verification\nor confirmation against the expected lot capacity and layout.\n&gt;&gt; User:\nStart a separate notification stream and notify me whenever there are more than 10 cars in the\nparking lots. Stop after 3 notifications.\n&gt;&gt; Assistant:\nI've started a separate notification stream to notify you whenever there are more than 10 cars in\nthe parking lots. This will stop after 3 notifications.",
    "crumbs": [
      "Demos",
      "Demo Scenario 3"
    ]
  },
  {
    "objectID": "demos/demo_scenario_1.html",
    "href": "demos/demo_scenario_1.html",
    "title": "Demo Scenario 1",
    "section": "",
    "text": "The goal of this demo is to demonstrate the utilities to dynamically import new tools via GPT Function calling, as well as incorporating some of our built-in tools in a relevant use case.\nThe scenario in this demo is a user wanting to retrieve information about traffic information. Without our application, the user needs to access an online service (e.g., Road DigiTraffic) about traffic information, navigate through the website UI, and making requests to the service with appropriate commands / UI navigation. However, the problem in this manual approach is that it can be time-consuming, prone to user error, and inefficient, especially if the user is unfamiliar with the website’s interface or the specific commands required. It is also difficult for the user to combine the information with other utilities.\nOur application can resolve these issues by dynamically importing API endpoints as tools for GPT FC. API endpoints are the backbone of an online service, so directly importing them as tools mean that GPT FC can operate all utilities available to the user. With this setup, GPT FC can interpret the user’s chat instructions, automatically operate the appropriate functions, and provide accurate, real-time traffic data efficiently. This simplifies the process of navigating through the utilities of a service and ensures that users can easily combine the information with other tools or workflows.",
    "crumbs": [
      "Demos",
      "Demo Scenario 1"
    ]
  },
  {
    "objectID": "demos/demo_scenario_1.html#step-1---preparation",
    "href": "demos/demo_scenario_1.html#step-1---preparation",
    "title": "Demo Scenario 1",
    "section": "Step 1 - Preparation",
    "text": "Step 1 - Preparation\nFor this demo, to fully demonstrate the dynamic imports via GPT FC, we will import only functions from llmcam.utils.store as built-in GPT FC tools. This function will be responsible for dynamically importing any tools that we use subsequently.\nAt this step, the main tasks are:\n\nImport llmcam.core modules and llmcam.utils.store module.\n\nSet up initial tools list and its handler as instructed in utils/01_store.ipynb notebook.\n\nVerify set up with GPT messages.\n\n\n# Import all the necessary modules\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import *\nfrom llmcam.utils.store import *\n\n\n# Set up `tools` list and `execute_handler` function\ntools = []\ndef execute_handler(function_name, **kwargs):\n    execute_handler_core(tools, function_name, **kwargs)\n\n# Add default tools from `llmcam.utils.store` built-in functions\ntools.extend([handler_schema(function, service_name=\"toolbox_handler\", fixup=execute_handler) for function in [\n    add_api_tools,\n    add_function_tools,\n    remove_tools\n]])\n\n\n# Start the conversation and verify the tools\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to help the user.\"),\n    (\"user\", \"What tools can you use?\")\n])\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI can add, remove, and manage tools related to API services and function tools within the\n`functions` namespace. I also have a capability to execute multiple tool operations in parallel\nusing the `multi_tool_use.parallel` function. If you need a specific API or functionality, let me\nknow, and I can manage the tools accordingly.\n\n\nThe GPT agent should list all the 3 built-in tools from llmcam.utils.store.",
    "crumbs": [
      "Demos",
      "Demo Scenario 1"
    ]
  },
  {
    "objectID": "demos/demo_scenario_1.html#step-2---integrate-road-digitraffic",
    "href": "demos/demo_scenario_1.html#step-2---integrate-road-digitraffic",
    "title": "Demo Scenario 1",
    "section": "Step 2 - Integrate Road DigiTraffic",
    "text": "Step 2 - Integrate Road DigiTraffic\nAt this step, we will integrate Road DigiTraffic via its base API service and OpenAPI Specification file. This service is public and users can retrieve traffic information by accessing its API endpoints with HTTPS requests. The official documentation is available at: https://www.digitraffic.fi/tieliikenne/. The base URL for making requests is https://tie.digitraffic.fi, while its OAS file is downloadable from the URL https://tie.digitraffic.fi/swagger/openapi.json.\nAs such, this service provides all information we need for incorporating it to GPT FC via llmcam.utils.store. It is also without the use of tokens or security schemes that our service is currently not accounting for. In this demo, we can use it to retrieve information about weather cameras and thereby incorporate further with our vision modules.\nThe main tasks for integrating Road DigiTraffic include:\n\nAdd Road DigiTraffic with necessary information.\n\nCheck for available tools.\n\nMake a request about weather camera stations.\n\n(Optional) Prune tool call results as response from Road DigiTraffic can exceed maximum tokens.\n\n\n# Add a new API service called 'road_digitraffic' with necessary details\nmessages.append(form_msg(\n    \"user\", \n    \"Add a new API service called 'road_digitraffic'. \\\nUse the base URL 'https://tie.digitraffic.fi', \\\nand the OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\"\n))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nThe new API service 'road_digitraffic' has been successfully added with the base URL\n'https://tie.digitraffic.fi' and the OpenAPI Specification URL\n'https://tie.digitraffic.fi/swagger/openapi.json'.\n\n\n\n# Verify the added API service\nmessages.append(form_msg(\"user\", \"What tools can you use?\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nI now have access to the following tools within the `functions` namespace:  1. **Road Digitraffic\nAPI Tools** (from the recently added 'road_digitraffic' service):    - Access various endpoints\nrelated to traffic measurement, weather stations, maintenance tracking, and more.  2. **Multi Tool\nExecution**:    - `multi_tool_use.parallel`: Execute multiple tools in parallel when they can\noperate independently.  If you need specific functionalities or data, feel free to ask, and I can\nuse the appropriate tool.\n\n\n\n# Make a request to the 'road_digitraffic' API service to get the traffic information\nmessages.append(form_msg(\"user\", \"Can you tell me where exists traffic jam right now?\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nThere is a traffic situation report for a traffic jam right now:  - **Location:** Road 6,\nLappeenranta, Finland - **Nature of Incident:** Preliminary accident report - **Specific Location:**\nBetween Laihian junction bridge and Muuko interchange - **Coordinates:** Path through multiple\npoints from [28.315513, 61.073364] to [28.348438, 61.084576] - **Start Time:** December 19, 2024,\n23:08:00 UTC - **Expected End Time:** December 19, 2024, 23:38:14 UTC - **Additional Information:**\n[Traffic Information and Road Conditions Online](https://liikennetilanne.fintraffic.fi/) -\n**Contact:** Fintraffic Tieliikennekeskus Tampere, Phone: 02002100, Email:\ntampere.liikennekeskus@fintraffic.fi  If you need more detailed information about other locations or\ntraffic situations, let me know!\n\n\n\nmessages = [ message for message in messages if message[\"role\"] != \"tool\" and message[\"content\"] != None ]",
    "crumbs": [
      "Demos",
      "Demo Scenario 1"
    ]
  },
  {
    "objectID": "demos/demo_scenario_1.html#step-3---integrate-llmcam.vision-modules",
    "href": "demos/demo_scenario_1.html#step-3---integrate-llmcam.vision-modules",
    "title": "Demo Scenario 1",
    "section": "Step 3 - Integrate llmcam.vision modules",
    "text": "Step 3 - Integrate llmcam.vision modules\nWe can combine the information about weather camera stations with our existing functions to capture images from weather cameras and detect objects in images. To demonstrate further the utilities from llmcam.utils.store, we can try dynamically importing this built-in functions with GPT FC.\nIn this step, the main tasks include:\n\nAdd necessary tools with full module source and function names llmcam.vision.dtcam.cap and llmcam.vision.yolo.detect_objects.\n\nMake command to capture image from a specified weather camera and detect objects from it.\n\n\n# Add the functions `llmcam.vision.dtcam.cap` and `llmcam.vision.yolo.detect_objects` to the new service 'vision'\nmessages.append(form_msg(\n    \"user\", \n    \"Add a new service called `vision` with the functions `llmcam.vision.dtcam.cap` and \\\n`llmcam.vision.yolo.detect_objects`.\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n&gt;&gt; Assistant:\nThe new service 'vision' has been added with the functions `llmcam.vision.dtcam.cap` and\n`llmcam.vision.yolo.detect_objects`. If you need to utilize these functions, feel free to ask!\n\n\n\n# Use the functions to show an image with a weather camera and detect objects in it\nmessages.append(form_msg(\n    \"user\", \n    \"Show me an image in Lappeenranta with a weather camera \\\nand detect objects in it.\"))\ncomplete(messages, tools=tools)\nprint_msg(messages[-1])\n\n\nimage 1/1 /home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.19_23:26:51_Imatralle_C0352901.jpg: 384x640 (no detections), 47.9ms\nSpeed: 6.6ms preprocess, 47.9ms inference, 51.8ms postprocess per image at shape (1, 3, 384, 640)\n&gt;&gt; Assistant:\nHere is an image captured from a weather camera in Lappeenranta:  ![Lappeenranta Weather\nCamera](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.19_23:26:51_Imatralle_C0352901.jpg)\nUnfortunately, no objects were detected in this image. If you have any other requests or need\nadditional information, feel free to ask!\n\n\n\n# Display the downloaded image\nfrom IPython.display import Image\nImage(\"/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.19_23:26:51_Imatralle_C0352901.jpg\")",
    "crumbs": [
      "Demos",
      "Demo Scenario 1"
    ]
  },
  {
    "objectID": "demos/demo_scenario_1.html#conversation-transcript",
    "href": "demos/demo_scenario_1.html#conversation-transcript",
    "title": "Demo Scenario 1",
    "section": "CONVERSATION TRANSCRIPT",
    "text": "CONVERSATION TRANSCRIPT\n\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to help the user.\n&gt;&gt; User:\nWhat tools can you use?\n&gt;&gt; Assistant:\nI can add, remove, and manage tools related to API services and function tools within the\n`functions` namespace. I also have a capability to execute multiple tool operations in parallel\nusing the `multi_tool_use.parallel` function. If you need a specific API or functionality, let me\nknow, and I can manage the tools accordingly.\n&gt;&gt; User:\nAdd a new API service called 'road_digitraffic'. Use the base URL 'https://tie.digitraffic.fi', and\nthe OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\n&gt;&gt; Assistant:\nThe new API service 'road_digitraffic' has been successfully added with the base URL\n'https://tie.digitraffic.fi' and the OpenAPI Specification URL\n'https://tie.digitraffic.fi/swagger/openapi.json'.\n&gt;&gt; User:\nWhat tools can you use?\n&gt;&gt; Assistant:\nI now have access to the following tools within the `functions` namespace:  1. **Road Digitraffic\nAPI Tools** (from the recently added 'road_digitraffic' service):    - Access various endpoints\nrelated to traffic measurement, weather stations, maintenance tracking, and more.  2. **Multi Tool\nExecution**:    - `multi_tool_use.parallel`: Execute multiple tools in parallel when they can\noperate independently.  If you need specific functionalities or data, feel free to ask, and I can\nuse the appropriate tool.\n&gt;&gt; User:\nCan you tell me where exists traffic jam right now?\n&gt;&gt; Assistant:\nThere is a traffic situation report for a traffic jam right now:  - **Location:** Road 6,\nLappeenranta, Finland - **Nature of Incident:** Preliminary accident report - **Specific Location:**\nBetween Laihian junction bridge and Muuko interchange - **Coordinates:** Path through multiple\npoints from [28.315513, 61.073364] to [28.348438, 61.084576] - **Start Time:** December 19, 2024,\n23:08:00 UTC - **Expected End Time:** December 19, 2024, 23:38:14 UTC - **Additional Information:**\n[Traffic Information and Road Conditions Online](https://liikennetilanne.fintraffic.fi/) -\n**Contact:** Fintraffic Tieliikennekeskus Tampere, Phone: 02002100, Email:\ntampere.liikennekeskus@fintraffic.fi  If you need more detailed information about other locations or\ntraffic situations, let me know!\n&gt;&gt; User:\nAdd a new service called `vision` with the functions `llmcam.vision.dtcam.cap` and\n`llmcam.vision.yolo.detect_objects`.\n&gt;&gt; Assistant:\nThe new service 'vision' has been added with the functions `llmcam.vision.dtcam.cap` and\n`llmcam.vision.yolo.detect_objects`. If you need to utilize these functions, feel free to ask!\n&gt;&gt; User:\nShow me an image in Lappeenranta with a weather camera and detect objects in it.\n&gt;&gt; Assistant:\nHere is an image captured from a weather camera in Lappeenranta:  ![Lappeenranta Weather\nCamera](sandbox:/home/nghivo/tinyMLaaS/llmcam/data/cap_2024.12.19_23:26:51_Imatralle_C0352901.jpg)\nUnfortunately, no objects were detected in this image. If you have any other requests or need\nadditional information, feel free to ask!",
    "crumbs": [
      "Demos",
      "Demo Scenario 1"
    ]
  },
  {
    "objectID": "utils/store.html",
    "href": "utils/store.html",
    "title": "Store",
    "section": "",
    "text": "While we have existing functions in core for constructing dynamic tools from locally installed functions or API endpoints, we need to incorporate these utilities into functions compatible with our current GPT Function Calling framework. There are some problems complicating this matter, including:\nTherefore, the goal in this notebook is to create a framework that allows a function to manage this top-level tools Python list via GPT Function calling. The function should extend the existing tools list or delete entries from this list. Considering that tools cannot be passed as Python instance in arguments of GPT tool call, we can utilize the fixup function to retrieve tools from global environment with appropriate metadata.",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/store.html#dynamic-function-installation",
    "href": "utils/store.html#dynamic-function-installation",
    "title": "Store",
    "section": "Dynamic function installation",
    "text": "Dynamic function installation\nSupposing that we have a Python function installed locally on our machine, it is possible to dynamically import with importlib.\n\nsource\n\nadd_function_tools\n\n add_function_tools (tools:list, service_name:str,\n                     function_names:list[str])\n\nAdd function tools to the toolbox.\n\n\n\n\nType\nDetails\n\n\n\n\ntools\nlist\nList of existing tools\n\n\nservice_name\nstr\nName of the service\n\n\nfunction_names\nlist\nList of function names (with module prefix)\n\n\n\nExample usage with some functions from llmcam.vision modules:\n\ntools = []\nadd_function_tools(\n    tools, \n    \"youtube_live\", \n    [\n        \"llmcam.vision.ytlive.capture_youtube_live_frame\", \n        \"llmcam.vision.gpt4v.ask_gpt4v_about_image_file\"\n    ])\nassert len(tools) == 2, \"Tools should be updated with 2 new function.\"\ntools\n\n[{'type': 'function',\n  'function': {'name': 'capture_youtube_live_frame',\n   'description': 'Capture a jpeg image from YouTube Live and return the path to the saved image',\n   'parameters': {'type': 'object',\n    'properties': {'link': {'anyOf': [{'type': 'string',\n        'description': 'YouTube Live link'},\n       {'type': 'null',\n        'description': 'A default value will be automatically used.'}]},\n     'place': {'anyOf': [{'type': 'string',\n        'description': 'Location of live image'},\n       {'type': 'null',\n        'description': 'A default value will be automatically used.'}]}},\n    'required': []},\n   'metadata': {'module': 'llmcam.vision.ytlive', 'service': 'youtube_live'}}},\n {'type': 'function',\n  'function': {'name': 'ask_gpt4v_about_image_file',\n   'description': 'Tell all about quantitative information from a given image file',\n   'parameters': {'type': 'object',\n    'properties': {'path': {'type': 'string',\n      'description': 'Path to the image file'}},\n    'required': ['path']},\n   'metadata': {'module': 'llmcam.vision.gpt4v', 'service': 'youtube_live'}}}]",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/store.html#dynamic-api-installation",
    "href": "utils/store.html#dynamic-api-installation",
    "title": "Store",
    "section": "Dynamic API installation",
    "text": "Dynamic API installation\nWith the functions in oas_to_schema, we can dynamically generate tools schema from API server with OpenAPI Specification file:\n\nsource\n\nadd_api_tools\n\n add_api_tools (tools:list, service_name:str, base_url:str,\n                oas_url:Optional[str]=None,\n                oas_destination:Optional[str]=None)\n\nAdd API tools to the toolbox.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntools\nlist\n\nList of existing tools\n\n\nservice_name\nstr\n\nName of the API service\n\n\nbase_url\nstr\n\nBase URL of the API service\n\n\noas_url\nOptional\nNone\nOpenAPI Specification URL\n\n\noas_destination\nOptional\nNone\nOpenAPI Specification destination file\n\n\n\nExample usage with Road DigiTraffic API:\n\ntools = []\nadd_api_tools(tools, \"road_digitraffic\", \"https://tie.digitraffic.fi\")\nassert len(tools) &gt; 0, \"Tools should be updated with new API tools.\"\ntools[0]\n\n{'type': 'function',\n 'function': {'name': 'tmsStationsDatex2Xml',\n  'description': 'The static information of TMS stations in Datex2 format (Traffic Measurement System / LAM)',\n  'parameters': {'type': 'object',\n   'properties': {'query': {'type': 'object',\n     'properties': {'state': {'type': 'string',\n       'description': 'Road station state',\n       'default': 'ACTIVE',\n       'enum': ['ALL', 'REMOVED', 'ACTIVE']}},\n     'required': []}},\n   'required': []},\n  'metadata': {'url': 'https://tie.digitraffic.fi/api/beta/tms-stations-datex2.xml',\n   'method': 'get',\n   'accepted_queries': ['state'],\n   'service': 'road_digitraffic'},\n  'fixup': 'llmcam.core.oas_to_schema.generate_request'}}",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/store.html#removal-of-existing-tools",
    "href": "utils/store.html#removal-of-existing-tools",
    "title": "Store",
    "section": "Removal of existing tools",
    "text": "Removal of existing tools\nCurrently, there is an upper limit for the number of tools (around 128). Therefore, to better manage the toolbox, we can add an utility function to remove tools of via service name.\n\nsource\n\nremove_tools\n\n remove_tools (tools:list, service_name:str)\n\nRemove tools from the toolbox.\n\n\n\n\nType\nDetails\n\n\n\n\ntools\nlist\nList of existing tools\n\n\nservice_name\nstr\nName of the service",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/store.html#fixup-setup-and-fc-framework",
    "href": "utils/store.html#fixup-setup-and-fc-framework",
    "title": "Store",
    "section": "Fixup setup and FC framework",
    "text": "Fixup setup and FC framework\nAs described previously, we can introduce a special fixup function is used for handlers which accesses the correct instance of tools. This function is to be created and attached to the tools instance at the moment of creation.\nBecause this fixup function should be attached to the tools instance during its creation, we should define a *_core function which operates the core operations for re-definition of execution function depending on the context rather than a fixed definition.\n\nsource\n\nexecute_handler_core\n\n execute_handler_core (tools:list, function_name:str, module:str,\n                       **kwargs)\n\nExecute the handler function by retrieving function with session ID.\n\n\n\n\nType\nDetails\n\n\n\n\ntools\nlist\nTools for each session\n\n\nfunction_name\nstr\nName of the function to execute\n\n\nmodule\nstr\nModule of the function\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\nsource\n\n\nhandler_schema\n\n handler_schema (function:Callable, service_name:str='toolbox_handler',\n                 fixup:Optional[Callable]=None, **kwargs)\n\nCreate a schema for handlers.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunction\nCallable\n\nHandler function\n\n\nservice_name\nstr\ntoolbox_handler\nName of the service\n\n\nfixup\nOptional\nNone\nFunction to fixup function execution\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\nExample usage with execution function adding tools from Road DigiTraffic:\n\ntools = []\n\n# Define the fixup function attached to this `tools` instance\ndef execute_handler(function_name, **kwargs):\n    execute_handler_core(tools, function_name, **kwargs)\n\n# Add the handler functions to the toolbox\ntools.extend([handler_schema(function, service_name=\"toolbox_handler\", fixup=execute_handler) for function in [\n        add_api_tools,\n        add_function_tools,\n        remove_tools\n    ]])\nassert len(tools) == 3, \"Tools should be updated with 3 new handler functions.\"\ntools[0]\n\n{'type': 'function',\n 'function': {'name': 'add_api_tools',\n  'description': 'Add API tools to the toolbox.',\n  'parameters': {'type': 'object',\n   'properties': {'service_name': {'type': 'string',\n     'description': 'Name of the API service'},\n    'base_url': {'type': 'string',\n     'description': 'Base URL of the API service'},\n    'oas_url': {'anyOf': [{'type': 'string',\n       'description': 'OpenAPI Specification URL'},\n      {'type': 'null',\n       'description': 'A default value will be automatically used.'}]},\n    'oas_destination': {'anyOf': [{'type': 'string',\n       'description': 'OpenAPI Specification destination file'},\n      {'type': 'null',\n       'description': 'A default value will be automatically used.'}]}},\n   'required': ['service_name', 'base_url']},\n  'metadata': {'module': '__main__', 'service': 'toolbox_handler'},\n  'fixup': '__main__.execute_handler'}}\n\n\n\nexecute_handler(\n    \"add_api_tools\",\n    module=\"__main__\",\n    service_name=\"road_digitraffic\",\n    base_url=\"https://tie.digitraffic.fi\"\n)\nassert len(tools) &gt; 3, \"Tools should be updated with new API tools.\"\ntools[4]\n\n{'type': 'function',\n 'function': {'name': 'tmsStationsDatex2Json',\n  'description': 'The static information of TMS stations in Datex2 format (Traffic Measurement System / LAM)',\n  'parameters': {'type': 'object',\n   'properties': {'query': {'type': 'object',\n     'properties': {'state': {'type': 'string',\n       'description': 'Road station state',\n       'default': 'ACTIVE',\n       'enum': ['ALL', 'REMOVED', 'ACTIVE']}},\n     'required': []}},\n   'required': []},\n  'metadata': {'url': 'https://tie.digitraffic.fi/api/beta/tms-stations-datex2.json',\n   'method': 'get',\n   'accepted_queries': ['state'],\n   'service': 'road_digitraffic'},\n  'fixup': 'llmcam.core.oas_to_schema.generate_request'}}",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/store.html#simulated-gpt-workflow",
    "href": "utils/store.html#simulated-gpt-workflow",
    "title": "Store",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework:\n\nfrom llmcam.core.fc import *\n\ntools = []\ndef execute_handler(function_name, **kwargs):\n    execute_handler_core(tools, function_name, **kwargs)\n\ntools.extend([handler_schema(function, service_name=\"toolbox_handler\", fixup=execute_handler) for function in [\n        add_api_tools,\n        add_function_tools,\n        remove_tools\n    ]])\nassert len(tools) == 3\n\n\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Add a new API service called 'road_digitraffic'. Use the base URL 'https://tie.digitraffic.fi', and the OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nAdd a new API service called 'road_digitraffic'. Use the base URL 'https://tie.digitraffic.fi', and\nthe OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\n&gt;&gt; Assistant:\nThe API service 'road_digitraffic' has been successfully added.\n\n\n\nlen(tools)\n\n65\n\n\n\nmessages.append(form_msg(\"user\", \"Get the weather camera information for the stations with ID C01503 and C01504.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nAdd a new API service called 'road_digitraffic'. Use the base URL 'https://tie.digitraffic.fi', and\nthe OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\n&gt;&gt; Assistant:\nThe API service 'road_digitraffic' has been successfully added.\n&gt;&gt; User:\nGet the weather camera information for the stations with ID C01503 and C01504.\n&gt;&gt; Assistant:\nHere is the weather camera information for the stations with IDs C01503 and C01504:  ### Station\nC01503 - kt51_Inkoo - **Coordinates**: [23.99616, 60.05374] - **Camera Type**: BOSCH - **Collection\nStatus**: GATHERING - **State**: FAULT_DOUBT - **Data Updated Time**: 2024-12-15T15:25:43Z -\n**Municipality**: Inkoo - **Province**: Uusimaa - **Purpose**: Weather Observation - **Presets**:\n- **Inkooseen**: ![View](https://weathercam.digitraffic.fi/C0150301.jpg)   - **Hankoon**:\n![View](https://weathercam.digitraffic.fi/C0150302.jpg)   - **Tienpinta**:\n![View](https://weathercam.digitraffic.fi/C0150309.jpg)  ### Station C01504 - vt2_Karkkila_Korpi -\n**Coordinates**: [24.235601, 60.536727] - **Camera Type**: HIKVISION - **Collection Status**:\nGATHERING - **Data Updated Time**: 2024-12-15T15:27:10Z - **Municipality**: Karkkila - **Province**:\nUusimaa - **Purpose**: Weather Observation - **Presets**:   - **Poriin**:\n![View](https://weathercam.digitraffic.fi/C0150401.jpg)   - **Helsinkiin**:\n![View](https://weathercam.digitraffic.fi/C0150402.jpg)   - **Tienpinta**:\n![View](https://weathercam.digitraffic.fi/C0150409.jpg)  Feel free to click on the image links to\nview the camera feed.\n\n\n\nmessages.append(form_msg(\"user\", \"Remove the 'road_digitraffic' service.\"))\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou are a helpful system administrator. Use the supplied tools to assist the user.\n&gt;&gt; User:\nAdd a new API service called 'road_digitraffic'. Use the base URL 'https://tie.digitraffic.fi', and\nthe OpenAPI Specification URL 'https://tie.digitraffic.fi/swagger/openapi.json'.\n&gt;&gt; Assistant:\nThe API service 'road_digitraffic' has been successfully added.\n&gt;&gt; User:\nGet the weather camera information for the stations with ID C01503 and C01504.\n&gt;&gt; Assistant:\nHere is the weather camera information for the stations with IDs C01503 and C01504:  ### Station\nC01503 - kt51_Inkoo - **Coordinates**: [23.99616, 60.05374] - **Camera Type**: BOSCH - **Collection\nStatus**: GATHERING - **State**: FAULT_DOUBT - **Data Updated Time**: 2024-12-15T15:25:43Z -\n**Municipality**: Inkoo - **Province**: Uusimaa - **Purpose**: Weather Observation - **Presets**:\n- **Inkooseen**: ![View](https://weathercam.digitraffic.fi/C0150301.jpg)   - **Hankoon**:\n![View](https://weathercam.digitraffic.fi/C0150302.jpg)   - **Tienpinta**:\n![View](https://weathercam.digitraffic.fi/C0150309.jpg)  ### Station C01504 - vt2_Karkkila_Korpi -\n**Coordinates**: [24.235601, 60.536727] - **Camera Type**: HIKVISION - **Collection Status**:\nGATHERING - **Data Updated Time**: 2024-12-15T15:27:10Z - **Municipality**: Karkkila - **Province**:\nUusimaa - **Purpose**: Weather Observation - **Presets**:   - **Poriin**:\n![View](https://weathercam.digitraffic.fi/C0150401.jpg)   - **Helsinkiin**:\n![View](https://weathercam.digitraffic.fi/C0150402.jpg)   - **Tienpinta**:\n![View](https://weathercam.digitraffic.fi/C0150409.jpg)  Feel free to click on the image links to\nview the camera feed.\n&gt;&gt; User:\nRemove the 'road_digitraffic' service.\n&gt;&gt; Assistant:\nThe 'road_digitraffic' service has been successfully removed.\n\n\n\n# After removing the 'road_digitraffic' service, the tools should only contain the initial handlers\nlen(tools)\n\n3",
    "crumbs": [
      "Utils",
      "Store"
    ]
  },
  {
    "objectID": "utils/bash_command.html",
    "href": "utils/bash_command.html",
    "title": "Execute bash commands",
    "section": "",
    "text": "We can execute any bash commands with Python subprocess library.\n\nsource\n\n\n\n execute_bash_command (command:str)\n\nExecute any given bash command with parameters\n\nres = execute_bash_command('whoami --help')\nprint(res)\n\nUsage: whoami [OPTION]...\nPrint the user name associated with the current effective user ID.\nSame as id -un.\n\n      --help     display this help and exit\n      --version  output version information and exit\n\nGNU coreutils online help: &lt;https://www.gnu.org/software/coreutils/&gt;\nReport any translation bugs to &lt;https://translationproject.org/team/&gt;\nFull documentation &lt;https://www.gnu.org/software/coreutils/whoami&gt;\nor available locally via: info '(coreutils) whoami invocation'",
    "crumbs": [
      "Utils",
      "Execute bash commands"
    ]
  },
  {
    "objectID": "utils/bash_command.html#execute-bash-command",
    "href": "utils/bash_command.html#execute-bash-command",
    "title": "Execute bash commands",
    "section": "",
    "text": "We can execute any bash commands with Python subprocess library.\n\nsource\n\n\n\n execute_bash_command (command:str)\n\nExecute any given bash command with parameters\n\nres = execute_bash_command('whoami --help')\nprint(res)\n\nUsage: whoami [OPTION]...\nPrint the user name associated with the current effective user ID.\nSame as id -un.\n\n      --help     display this help and exit\n      --version  output version information and exit\n\nGNU coreutils online help: &lt;https://www.gnu.org/software/coreutils/&gt;\nReport any translation bugs to &lt;https://translationproject.org/team/&gt;\nFull documentation &lt;https://www.gnu.org/software/coreutils/whoami&gt;\nor available locally via: info '(coreutils) whoami invocation'",
    "crumbs": [
      "Utils",
      "Execute bash commands"
    ]
  },
  {
    "objectID": "utils/bash_command.html#simulated-gpt-workflow",
    "href": "utils/bash_command.html#simulated-gpt-workflow",
    "title": "Execute bash commands",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating this into current GPT framework with multiple commands:\n\nfrom llmcam.core.fc import *\nfrom llmcam.core.fn_to_schema import function_schema\n\ntools = [function_schema(execute_bash_command)]\n\n\nmessages = form_msgs([\n    (\"system\", \"You are a helpful system administrator. Use the supplied tools to assist the user.\"),\n    (\"user\", \"Show the disk usage with bash command\"),  \n])\ncomplete(messages, tools=tools)\n\n('assistant',\n 'Here is the current disk usage for your system:\\n\\n```\\nFilesystem      Size  Used Avail Use% Mounted on\\nnone            2.9G  4.0K  2.9G   1% /mnt/wsl\\ndrivers         475G  422G   53G  89% /usr/lib/wsl/drivers\\nnone            2.9G     0  2.9G   0% /usr/lib/modules\\nnone            2.9G     0  2.9G   0% /usr/lib/modules/5.15.153.1-microsoft-standard-WSL2\\n/dev/sdc       1007G   23G  933G   3% /\\nnone            2.9G   88K  2.9G   1% /mnt/wslg\\nnone            2.9G     0  2.9G   0% /usr/lib/wsl/lib\\nrootfs          2.9G  2.1M  2.9G   1% /init\\nnone            2.9G  848K  2.9G   1% /run\\nnone            2.9G     0  2.9G   0% /run/lock\\nnone            2.9G     0  2.9G   0% /run/shm\\ntmpfs           4.0M     0  4.0M   0% /sys/fs/cgroup\\nnone            2.9G   76K  2.9G   1% /mnt/wslg/versions.txt\\nnone            2.9G   76K  2.9G   1% /mnt/wslg/doc\\nC:\\\\\\\\             475G  422G   53G  89% /mnt/c\\nsnapfuse        128K  128K     0 100% /snap/bare/5\\nsnapfuse         74M   74M     0 100% /snap/core22/1722\\nsnapfuse         74M   74M     0 100% /snap/core22/1663\\nsnapfuse         92M   92M     0 100% /snap/gtk-common-themes/1535\\nsnapfuse         39M   39M     0 100% /snap/snapd/21759\\nsnapfuse         45M   45M     0 100% /snap/snapd/23258\\nsnapfuse        132M  132M     0 100% /snap/ubuntu-desktop-installer/1276\\nsnapfuse        132M  132M     0 100% /snap/ubuntu-desktop-installer/1286\\n```\\n\\nThis output shows the size, used space, available space, and usage percentage of each filesystem.')\n\n\n\nmessages.append(form_msg(\"user\", \"show the system info with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n \"Here's the system information:\\n\\n```\\nLinux e15 6.8.0-49-generic #49~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux\\n```\\n\\nThis output includes the kernel name (`Linux`), the hostname (`e15`), the kernel version (`6.8.0-49-generic`), the architecture (`x86_64`), and other system details related to your current Ubuntu setup.\")\n\n\n\nmessages.append(form_msg(\"user\", \"tell current directory with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant', 'The current directory is:\\n\\n```\\n/home/doyu/00llmcam/nbs\\n```')\n\n\n\nmessages.append(form_msg(\"user\", \"list files under ../data with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'Here are the files under the `../data` directory:\\n\\n- `cap_2024.09.28_15:59:06_Presidentinlinna.jpg`\\n- `cap_2024.09.28_16:00:11_Presidentinlinna.jpg`\\n- `cap_2024.09.28_16:01:16_Etelasatama.jpg`\\n- `cap_2024.09.28_16:02:21_Etelasatama.jpg`\\n- `cap_2024.09.28_16:05:31_Olympiaterminaali.jpg`\\n- `cap_2024.09.28_16:06:36_Olympiaterminaali.jpg`\\n- `cap_2024.09.28_16:07:41_Torni.jpg`\\n- `cap_2024.09.28_16:09:51_Tuomiokirkko.jpg`\\n- `cap_2024.09.28_16:15:11_Presidentinlinna.jpg`\\n- `cap_2024.09.28_17:33:31_Presidentinlinna.jpg`\\n- ... (and many more)\\n\\nIf you need to see a specific part of the list or need further details, please let me know!')\n\n\n\nmessages.append(form_msg(\"user\", \"rename cap_2024.09.28_15:59:06_Presidentinlinna.jpg with xcap_2024.09.28_15:59:06_Presidentinlinna.jpg with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The file has been successfully renamed from `cap_2024.09.28_15:59:06_Presidentinlinna.jpg` to `xcap_2024.09.28_15:59:06_Presidentinlinna.jpg`.')\n\n\n\nmessages.append(form_msg(\"user\", \"check if there's xcap_2024.09.28_15:59:06_Presidentinlinna.jpg with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The file `xcap_2024.09.28_15:59:06_Presidentinlinna.jpg` does exist in the `../data` directory.')\n\n\n\nmessages.append(form_msg(\"user\", \"show some of the running processes with bash command\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'Here are some of the running processes:\\n\\n```\\nUSER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\\nroot           1  0.0  0.0 167064 12084 ?        Ss   Nov30   0:03 /sbin/init splash\\nroot           2  0.0  0.0      0     0 ?        S    Nov30   0:00 [kthreadd]\\nroot           3  0.0  0.0      0     0 ?        S    Nov30   0:00 [pool_workqueue_release]\\nroot           4  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/R-rcu_g]\\nroot           5  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/R-rcu_p]\\nroot           6  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/R-slub_]\\nroot           7  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/R-netns]\\nroot          10  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/0:0H-events_highpri]\\nroot          12  0.0  0.0      0     0 ?        I&lt;   Nov30   0:00 [kworker/R-mm_pe]\\n```\\n\\nThis output includes the user, process ID (PID), CPU and memory usage, process status, and the command that initiated the process.')\n\n\n\nmessages.append(form_msg(\"user\", \"list only those process ids\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'Here are some of the process IDs (PIDs) of running processes:\\n\\n```\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n10\\n12\\n13\\n```')\n\n\n\nmessages.append(form_msg(\"user\", \"What kind of Openrating System is running?\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The operating system running is a version of Linux, specifically:\\n\\n```\\nUbuntu, with Linux kernel version 6.8.0-49-generic\\n```\\n\\nThis information indicates that the machine is running Ubuntu as its operating system.')\n\n\n\nmessages.append(form_msg(\"user\", \"Capture the desktop image\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n \"It seems there was an error trying to capture the desktop image. The tool could not access the X Window system, possibly due to the environment in which it's running, which may not support direct graphical access or the X server is not available.\\n\\nIf you are running in a headless environment or without a graphical interface, this could lead to such errors. Let me know if there's anything else you need help with!\")\n\n\n\nmessages.append(form_msg(\"user\", \"install scrot\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The `scrot` package is already installed and is the newest version available. You can use it to capture screenshots. If you need further assistance with capturing the screen using `scrot`, feel free to ask!')\n\n\n\nmessages.append(form_msg(\"user\", \"Capture the desktop image\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The desktop image has been successfully captured and saved as `desktop_capture.png` in the `../data` directory.')\n\n\n\nmessages.append(form_msg(\"user\", \"uninstall scrot\"))\ncomplete(messages, tools=tools)\n\n('assistant',\n 'The `scrot` package has been successfully uninstalled from your system. If you wish to clean up other packages that were automatically installed with `scrot` and are no longer needed, you can run `sudo apt autoremove`. If you need further assistance, feel free to ask!')",
    "crumbs": [
      "Utils",
      "Execute bash commands"
    ]
  },
  {
    "objectID": "core/fn_to_schema.html",
    "href": "core/fn_to_schema.html",
    "title": "Function to Tool Schema",
    "section": "",
    "text": "To execute function calling from GPT responses, we need to define the function and its schema as a GPT tool. However, it can be inconvenient to define both, especially if we need to incorporate multiple functions as GPT tools.\nThis process can be simplified if we utilize type-hinting and annotations on functions supported by Python using libraries such as inspect and ast. Summary of our schema generation process:\nLet us start with a well-documented function get_weather_information:\n# Define the function to get weather information\nfrom typing import Optional\n\ndef get_weather_information(\n    city: str,  # Name of the city\n    zip_code: Optional[str] = None,  # Zip code of the city (optional)\n):\n    \"\"\"Get weather information for a city or location based on zip code\"\"\"\n    return {\n        \"city\": city,\n        \"zip_code\": zip_code,\n        \"temparature\": 25,\n        \"humidity\": 80,\n    }",
    "crumbs": [
      "Core",
      "Function to Tool Schema"
    ]
  },
  {
    "objectID": "core/fn_to_schema.html#parameter-descriptions",
    "href": "core/fn_to_schema.html#parameter-descriptions",
    "title": "Function to Tool Schema",
    "section": "Parameter descriptions",
    "text": "Parameter descriptions\nWe can extract the descriptions of function parameters with the ast library. In our implementation, we can follow the inline comments for conveniency. The descriptions can be extracted as follows:\n\nsource\n\nextract_parameter_comments\n\n extract_parameter_comments (func:Callable)\n\nExtract comments for function arguments\n\n\n\n\nType\nDetails\n\n\n\n\nfunc\nCallable\nFunction to extract comments from\n\n\nReturns\ndict\nDictionary with parameter comments\n\n\n\nExample of extracting comments from the function:\n\nextract_parameter_comments(get_weather_information)\n\n{'city': 'Name of the city', 'zip_code': 'Zip code of the city (optional)'}",
    "crumbs": [
      "Core",
      "Function to Tool Schema"
    ]
  },
  {
    "objectID": "core/fn_to_schema.html#type-converter",
    "href": "core/fn_to_schema.html#type-converter",
    "title": "Function to Tool Schema",
    "section": "Type converter",
    "text": "Type converter\nPython types cannot be directly transferred into acceptable data types in GPT-compatible tool schema. Therefore, we need an utility to convert these types:\n\nsource\n\nparam_converter\n\n param_converter (param_type, description)\n\nConvert Python parameter types to acceptable types for tool schema\n\n\n\n\nType\nDetails\n\n\n\n\nparam_type\n\nThe type of the parameter\n\n\ndescription\n\nThe description of the parameter\n\n\nReturns\ndict\nThe converted parameter\n\n\n\nTest with parameters of get_weather_information:\n\ncity_param = param_converter(str, \"Name of the city\")\nzip_param = param_converter(Optional[str], \"Zip code of the city (optional)\")\ncity_param, zip_param\n\n({'type': 'string', 'description': 'Name of the city'},\n {'anyOf': [{'type': 'string',\n    'description': 'Zip code of the city (optional)'},\n   {'type': 'null',\n    'description': 'A default value will be automatically used.'}]})",
    "crumbs": [
      "Core",
      "Function to Tool Schema"
    ]
  },
  {
    "objectID": "core/fn_to_schema.html#function-to-schema",
    "href": "core/fn_to_schema.html#function-to-schema",
    "title": "Function to Tool Schema",
    "section": "Function to Schema",
    "text": "Function to Schema\nWe can combine the above utilities with other utilities in inspect to extract information from a Python function and generate a tool schema.\n\nsource\n\nfunction_schema\n\n function_schema (func:Callable, service_name:Optional[str]=None,\n                  fixup:Optional[Callable]=None, **kwargs)\n\nGenerate a schema from function using its parameters and docstring\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfunc\nCallable\n\nThe function to generate the schema for\n\n\nservice_name\nOptional\nNone\nThe name of the service\n\n\nfixup\nOptional\nNone\nA function to fix up the schema\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\ndict\n\nThe generated tool schema\n\n\n\nTest with our current function:\n\ntool_schema = function_schema(get_weather_information, service_name=\"Weather Service\")\ntool_schema\n\n{'type': 'function',\n 'function': {'name': 'get_weather_information',\n  'description': 'Get weather information for a city or location based on zip code',\n  'parameters': {'type': 'object',\n   'properties': {'city': {'type': 'string',\n     'description': 'Name of the city'},\n    'zip_code': {'anyOf': [{'type': 'string',\n       'description': 'Zip code of the city (optional)'},\n      {'type': 'null',\n       'description': 'A default value will be automatically used.'}]}},\n   'required': ['city']},\n  'metadata': {'module': '__main__', 'service': 'Weather Service'}}}",
    "crumbs": [
      "Core",
      "Function to Tool Schema"
    ]
  },
  {
    "objectID": "core/fn_to_schema.html#simulated-gpt-workflow",
    "href": "core/fn_to_schema.html#simulated-gpt-workflow",
    "title": "Function to Tool Schema",
    "section": "Simulated GPT workflow",
    "text": "Simulated GPT workflow\nTest integrating with our current GPT framework:\n\nfrom llmcam.core.fc import *\n\ntools = [function_schema(get_weather_information, service_name=\"Weather Service\")]\nmessages = form_msgs([\n    (\"system\", \"You can get weather information for a given location using the `get_weather_information` function\"),\n    (\"user\", \"What is the weather in New York?\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages)\n\n&gt;&gt; System:\nYou can get weather information for a given location using the `get_weather_information` function\n&gt;&gt; User:\nWhat is the weather in New York?\n&gt;&gt; Assistant:\nThe current weather in New York is 25°C with 80% humidity.",
    "crumbs": [
      "Core",
      "Function to Tool Schema"
    ]
  },
  {
    "objectID": "core/fc_exec.html",
    "href": "core/fc_exec.html",
    "title": "Function Calling",
    "section": "",
    "text": "Summary of GPT Function calling process as implemented in this module:",
    "crumbs": [
      "Core",
      "Function Calling"
    ]
  },
  {
    "objectID": "core/fc_exec.html#gpt-responses---schema-to-tool-calls",
    "href": "core/fc_exec.html#gpt-responses---schema-to-tool-calls",
    "title": "Function Calling",
    "section": "GPT responses - Schema to Tool calls",
    "text": "GPT responses - Schema to Tool calls\nBy default, OpenAI GPT accounts for its ‘tools’ and understands them via an inputted tools schema. Based on the description in the provided schema, GPT selects appropriate tools to answer to the user’s messages. However, GPT does not execute these tools directly, but only provides a response message that contains the name and arguments for the selected tools.\nAn example using GPT with a tool for getting weather information:\n\n# Define the function to get weather information\nfrom typing import Optional\n\ndef get_weather_information(\n    city: str,\n    zip_code: Optional[str] = None,\n):\n    return {\n        \"city\": city,\n        \"zip_code\": zip_code,\n        \"temparature\": 25,\n        \"humidity\": 80,\n    }\n\n\n# Define the tools schema with 'get_weather_information' function\ntools = [{\n    'type': 'function',\n    'function': {\n        'name': 'get_weather_information',\n        'description': 'Get weather information for a given location',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'city': {\n                    'type': 'string',\n                    'description': 'City name'\n                },\n                'zip_code': {\n                    'anyOf': [\n                        {'type': 'string'},\n                        {'type': 'null'},\n                    ],\n                },\n            },\n            'required': ['city'],\n        },\n    }\n}]\n\n\n# Generate the response using the chat API\nimport openai\nfrom pprint import pprint\n\nmessages = [\n    {\n        'role': 'system',\n        'content': 'You can get weather information for a given location using the `get_weather_information` function',\n    },\n    {\n        'role': 'user',\n        'content': 'What is the weather in New York?',\n    }\n]\n\nresponse = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, tools=tools)\npprint(response.choices[0].message.to_dict())\n\n{'content': None,\n 'refusal': None,\n 'role': 'assistant',\n 'tool_calls': [{'function': {'arguments': '{\"city\":\"New York\"}',\n                              'name': 'get_weather_information'},\n                 'id': 'call_OM0VepmBDaPN6TbUd4P9lXur',\n                 'type': 'function'}]}\n\n\nThis example illustrates the raw response from GPT for a tool call and related-information that can be retrieved from it. Essentially, we can extract:\n\nFunction name: alpha-numeric string unique for each tool in tools schema\n\nArguments: Inputs to feed into the function presented in key-value pairs\n\nApart from function-specific information, we also obtain an ID for this tool call. This is necessary to match this tool call to its results and generate further messages, as demonstrated in the following parts.\n\n# Get the function name and parameters from the response\ntool_calls = response.choices[0].message.to_dict()['tool_calls']\nfunction_name = tool_calls[0]['function']['name']\nfunction_parameters = tool_calls[0]['function']['arguments']\ntool_call_id = tool_calls[0]['id']\n\nfunction_name, function_parameters, tool_call_id\n\n('get_weather_information',\n '{\"city\":\"New York\"}',\n 'call_OM0VepmBDaPN6TbUd4P9lXur')\n\n\n\n# Call the function with the parameters\nimport json\n\nfunction = globals()[function_name]\nresults = function(**json.loads(function_parameters))\npprint(results)\n\n{'city': 'New York', 'humidity': 80, 'temparature': 25, 'zip_code': None}\n\n\nWe can continue the previous conversation by adding both the tool call initiated by GPT and the results of such tool call:\n\nmessages.append(response.choices[0].message.to_dict())\nmessages.append({\n    'role': 'tool',\n    'content': json.dumps({\n        **json.loads(function_parameters),\n        function_name: results,\n    }),\n    'tool_call_id': tool_call_id,\n})\n\n\nresponse = openai.chat.completions.create(model=\"gpt-4o\", messages=messages, tools=tools)\npprint(response.choices[0].message.to_dict())\n\n{'content': 'The current weather in New York is 25°C with a humidity level of '\n            '80%.',\n 'refusal': None,\n 'role': 'assistant'}\n\n\nThis overall workflow should be the guideline for us to implement the function to execute function calling by following its steps:\n\nGenerate tool call(s) with GPT response\nAdd tool call to conversation (messages)\nExecute tool call in system with the given name and arguments\nAdd results of tool call with corresponding ID to conversation\nRe-generate message with the updated conversation",
    "crumbs": [
      "Core",
      "Function Calling"
    ]
  },
  {
    "objectID": "core/fc_exec.html#execute-tool-call-in-system-global-environment",
    "href": "core/fc_exec.html#execute-tool-call-in-system-global-environment",
    "title": "Function Calling",
    "section": "Execute tool call in System / Global environment",
    "text": "Execute tool call in System / Global environment\nIn the above workflow, the actual function can be retrieved from global environment via its name. However, this approach is not appropriate for functions imported from other modules or API requests. Therefore, we can consider dynamic imports / function extraction and wrapper functions. These can be managed in data defined in higher levels of tools schema - metadata.\n\nDynamic imports\nWe can dynamically import functions and modules with importlib. To achieve this, we need module source as string combined with function name. For example, the function get_weather_information defined locally can be imported from module __main__. Meanwhile, show_doc function can be dynamically imported from module nbdev.showdoc.\n\nimport importlib\n\n# Example of dynamically calling the 'get_weather_information' function\nmain_module = importlib.import_module('__main__')\nweather_function = getattr(main_module, 'get_weather_information')\nweather_function(**{'city': 'New York'})\n\n{'city': 'New York', 'zip_code': None, 'temparature': 25, 'humidity': 80}\n\n\n\n# Example of dynamically calling the 'show_doc' function\nshowdoc_module = importlib.import_module('nbdev.showdoc')\nshowdoc_function = getattr(showdoc_module, 'show_doc')\nshowdoc_function(weather_function)\n\n\n\nget_weather_information\n\n get_weather_information (city:str, zip_code:Optional[str]=None)\n\n\n\n\n\n\nWrapper function\nIn case the main function fails, we can execute resort to a wrapper function that also takes in high-level data. This should be useful for any scenarios that require additional information to execute or specific steps (e.g., API requests). For system design purpose, these fixup functions should include the following parameters:\n\nFunction name: Positional, required paramater\n\nMetadata parameters: Any high-level data as optional keyword arguments\n\nFunction parameters: Function arguments provided by GPT as optional keyword arguments\n\n\n\nHigh-level data in tools schema\nHigh-level data should be stored as simple JSON-formatted data in tools schema such that it does not interfere with GPT argument-generating process. A suitable structure for this would be adding these information as properties in the function object of schema:\n\nmetadata: JSON-formatted data for any additional information\n\nfixup: Name and module source of fixup function\n\nExample of tools schema with high-level data:\n\n# Define the tools schema high-level information\ntools = [{\n    'type': 'function',\n    'function': {\n        'name': 'get_weather_information',\n        'description': 'Get weather information for a given location',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'city': {\n                    'type': 'string',\n                    'description': 'City name'\n                },\n                'zip_code': {\n                    'anyOf': [\n                        {'type': 'string'},\n                        {'type': 'null'},\n                    ],\n                },\n            },\n            'required': ['city'],\n        },\n        # Extra high-level information\n        'metadata': {\n            'module': '__main__',  # Module name\n        },\n        'fixup': 'fixup.module.function',  # Fixup function\n    }\n}]",
    "crumbs": [
      "Core",
      "Function Calling"
    ]
  },
  {
    "objectID": "core/fc_exec.html#utilities-function-for-managing-messages",
    "href": "core/fc_exec.html#utilities-function-for-managing-messages",
    "title": "Function Calling",
    "section": "Utilities function for managing messages",
    "text": "Utilities function for managing messages\nThis section implements some basic utilities for forming and printing messages to suitable formats used in conversations.\n\nsource\n\nform_msg\n\n form_msg (role:Literal['system','user','assistant','tool'], content:str,\n           tool_call_id:Optional[str]=None)\n\nCreate a message for the conversation\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nrole\nLiteral\n\nThe role of the message sender\n\n\ncontent\nstr\n\nThe content of the message\n\n\ntool_call_id\nOptional\nNone\nThe ID of the tool call (if role == “tool”)\n\n\n\n\nsource\n\n\nform_msgs\n\n form_msgs\n            (msgs:list[tuple[typing.Literal['system','user','assistant'],s\n            tr]])\n\nForm a list of messages for the conversation\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nmsgs\nlist\nThe list of messages to form in tuples of role and content\n\n\n\n\nsource\n\n\nprint_msg\n\n print_msg (msg:dict)\n\nPrint a message with role and content\n\n\n\n\nType\nDetails\n\n\n\n\nmsg\ndict\nThe message to print with role and content\n\n\n\n\nsource\n\n\nprint_msgs\n\n print_msgs (msgs:list[dict], with_tool:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\nlist\n\nThe list of messages to print with role and content\n\n\nwith_tool\nbool\nFalse\nWhether to print tool messages",
    "crumbs": [
      "Core",
      "Function Calling"
    ]
  },
  {
    "objectID": "core/fc_exec.html#modularized-execution-function",
    "href": "core/fc_exec.html#modularized-execution-function",
    "title": "Function Calling",
    "section": "Modularized execution function",
    "text": "Modularized execution function\nThis section implements the described FC workflow in a thorough execution function.\n\nsource\n\nfn_result_content\n\n fn_result_content (call, tools=[])\n\nCreate a content containing the result of the function call\n\nsource\n\n\nfn_exec\n\n fn_exec (call, tools=[])\n\nExecute the function call\n\nsource\n\n\nfn_metadata\n\n fn_metadata (tool)\n\n\nsource\n\n\nfn_args\n\n fn_args (call)\n\n\nsource\n\n\nfn_name\n\n fn_name (call)\n\n\nsource\n\n\ncomplete\n\n complete (messages:list[dict], tools:list[dict]=[])\n\nComplete the conversation with the given message\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmessages\nlist\n\nThe list of messages\n\n\ntools\nlist\n[]\nThe list of tools\n\n\nReturns\ntuple\n\nThe role and content of the last message\n\n\n\nTest with the previously demonstrated example - using the get_weather_information function and updated tools with metadata containing the module:\n\nmessages = form_msgs([\n    (\"system\", \"You can get weather information for a given location using the `get_weather_information` function\"),\n    (\"user\", \"What is the weather in New York?\")\n])\ncomplete(messages, tools=tools)\nprint_msgs(messages, with_tool=True)\n\n&gt;&gt; System:\nYou can get weather information for a given location using the `get_weather_information` function\n&gt;&gt; User:\nWhat is the weather in New York?\n&gt;&gt; Assistant:\n{'content': None, 'refusal': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_D819bD5iC06S3OmMGUIjuZi9', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_weather_information'}, 'type': 'function'}]}\n&gt;&gt; Tool:\n{\"city\": \"New York\", \"get_weather_information\": {\"city\": \"New York\", \"zip_code\": null,\n\"temparature\": 25, \"humidity\": 80}}\n&gt;&gt; Assistant:\nThe current weather in New York is 25°C with a humidity level of 80%.",
    "crumbs": [
      "Core",
      "Function Calling"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "llmcam",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "llmcam"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "llmcam",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall llmcam in Development mode\n# make sure llmcam package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to llmcam\n$ nbdev_prepare",
    "crumbs": [
      "llmcam"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "llmcam",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/ninjalabo/llmcam.git\nor from conda\n$ conda install -c ninjalabo llmcam\nor from pypi\n$ pip install llmcam\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "llmcam"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "llmcam",
    "section": "How to use",
    "text": "How to use\nIn development mode, navigate to the local repository and install the editable version:\n$ cd /path/to/llmcam/repository\n$ pip install -e . ['dev']\nSet up environment with necessary variables:\n\nOPENAI_API_KEY: Key for using GPT chat completion API from OpenAI.\n\nLLMCAM_DATA: Local directory for saving generated images to.\n\nStart by importing our modules:\n\nfrom llmcam.vision.ytlive import NHsta\nfrom llmcam.vision.gpt4v import ask_gpt4v_about_image_file\nfrom llmcam.application.runner import run_llmcam\n\n\nfn = NHsta()()\nfn\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=LMZQ7eFhm58\n[youtube] LMZQ7eFhm58: Downloading webpage\n[youtube] LMZQ7eFhm58: Downloading ios player API JSON\n[youtube] LMZQ7eFhm58: Downloading web creator player API JSON\n[youtube] LMZQ7eFhm58: Downloading m3u8 information\n11.11.2024 17:50:38\ncap_2024.11.11_17:58:09_unclear.jpg\n\n\nPath('../data/cap_2024.11.11_17:58:09_unclear.jpg')\n\n\n\nask_gpt4v_about_image_file(fn)\n\n{'timestamp': '2024-10-30T21:55:57',\n 'location': 'Tuomiokirkko',\n 'dimensions': {'width': 1280, 'height': 720},\n 'buildings': {'number_of_buildings': 15,\n  'building_height_range': '2-5 stories'},\n 'vehicles': {'number_of_vehicles': 0},\n 'waterbodies': {'visible': False},\n 'street_lights': {'number_of_street_lights': 25},\n 'people': {'approximate_number': 0},\n 'lighting': {'time_of_day': 'night', 'artificial_lighting': 'prominent'},\n 'visibility': {'clear': True},\n 'sky': {'visible': True, 'light_conditions': 'night'}}\n\n\n\n# Start the application on port 5001\nrun_llmcam()",
    "crumbs": [
      "llmcam"
    ]
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "llmcam",
    "section": "Modules",
    "text": "Modules\nThe application consists of 4 main modules:\n\ncore: Core module for implementing GPT Function calling (FC)\n\nutils: Module for implementing utilities as compatible functions for GPT FC\n\nvision: Module for implementing computer vision features as compatible functions for GPT FC\n\napplication: Module for implementing web-service",
    "crumbs": [
      "llmcam"
    ]
  },
  {
    "objectID": "application/runner.html",
    "href": "application/runner.html",
    "title": "Runner",
    "section": "",
    "text": "In addition to the main app, an utility function is implemented to run the app just by importing and executing this function to a Python file.\nWe can implement the runner based on actual implementation of JuViApp:\n\nsource\n\nrun_llmcam\n\n run_llmcam (host='0.0.0.0', port=5001, data_path=None, openai_key=None)\n\nRun the LLMCAM chatbot application\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhost\nstr\n0.0.0.0\nThe host to listen on\n\n\nport\nint\n5001\nThe port to listen on\n\n\ndata_path\nNoneType\nNone\nThe path to the data directory\n\n\nopenai_key\nNoneType\nNone\nThe OpenAI API key",
    "crumbs": [
      "Application",
      "Runner"
    ]
  }
]